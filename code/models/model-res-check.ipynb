{
 "metadata": {
  "name": "",
  "signature": "sha256:57fe7a598dd7d28f72ee703e325f2271a5eed17e97d6d5fb518f32beb47c1cc2"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Check full model code resolution errors, all SNRs"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our numerical full model code carries a resolution error, which approaches zero as our resolution is increased (i.e., our discrete code converges towards an exact solution [granted, considering each discretization/mesh separately]).  But, without an analytic solution, we can't compute the error directly (if we had the analytic solution we'd just use that, anyways).\n",
      "\n",
      "We estimate resolution error un-rigorously by computing fractional error, increasing resolution until fractional error is smaller than our tolerance, and praying that the absolute error (sum of fractional errors) will be small enough at the end.\n",
      "\n",
      "It is worth thinking about this more deeply, but it's unfortunately a low priority right now. and our model is a bit of a black box.  I imagine we could look at the integrals and compute error bounds on each one analytically, then propagate the errors through somehow.  Might be good as an exercise.\n",
      "\n",
      "Caveat: we are only looking at fractional error in FWHMs, we have not considered fractional error in best-fit parameters ($B_0$, $\\eta_2$)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "README"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "How to use: set configuration cells near the top, adding in appropriate numbers for measured widths / SNR settings / etc.  Change base fit kwargs to check resolutions for damping, change default settings to compare to, etc.\n",
      "\n",
      "After making sure your parameter \"grid\" is good, just run the whole notebook and record your observations at the end.  It should take a minute or two to run."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "1. Imports, set-up fitters for computation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import division\n",
      "\n",
      "from IPython import parallel\n",
      "\n",
      "import cPickle as pickle\n",
      "import numpy as np\n",
      "\n",
      "import fullmodel_recompile\n",
      "fullmodel_recompile.main()\n",
      "\n",
      "import models\n",
      "import models_exec as mex\n",
      "from models_exec import Fitter\n",
      "import parutils\n",
      "import snr_catalog as snrcat\n",
      "\n",
      "%load_ext autoreload\n",
      "%autoreload 2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Parallelize to speed up computation (remember to start the cluster first)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lview = parutils.get_lview()\n",
      "%px %load_ext autoreload\n",
      "%px autoreload 2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "2. Assemble data and parameters to check"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "2.1 Some setup"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Utilities to create `Fitter()` objects and manipulate SNR settings being investigated"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Bounding parameters are determined by trial and error to give FWHMs outside of FWHM bounds.\n",
      "Simply use a `Fitter()` object's `width_full` method (merely a wrapper for `models.width_full`).\n",
      "\n",
      "Parameters are akin to tables: dicts keyed by mu, then eta2, then a list of min/max B0 values.\n",
      "I input B0 values in $\\mu\\mathrm{G}$ and then rescale, for clarity/convenience."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def rescale_B0(p):\n",
      "    for mu in p:\n",
      "        for eta2 in p[mu]:\n",
      "            p[mu][eta2] = 1e-6 * np.array(p[mu][eta2])\n",
      "    return p\n",
      "\n",
      "def init_par_bounds(fobj, base_kws, pars, fwhms_min, fwhms_max):\n",
      "    \"\"\"Check that parameter bounds are valid, return dict of FWHMs\"\"\"\n",
      "    \n",
      "    print 'Computing all FWHMs'\n",
      "    par_tups = []\n",
      "    for mu in pars:\n",
      "        for eta2 in pars[mu]:\n",
      "            B_min, B_max = pars[mu][eta2]\n",
      "            par_tups.append((mu, eta2, B_min))\n",
      "            par_tups.append((mu, eta2, B_max))\n",
      "    \n",
      "    fwhms = lview.map_async(lambda ptup: fobj.width_full(*ptup, **base_kws), par_tups)\n",
      "    parutils.stdout_when_done(fwhms)\n",
      "    print 'Time elapsed: {}'.format(fwhms.elapsed)\n",
      "    \n",
      "    fwhm_dict = dict(zip(par_tups, fwhms))\n",
      "    \n",
      "    print 'Checking for valid parameter bounds'\n",
      "    flag = False\n",
      "    for ptup in par_tups:\n",
      "        if any(fwhms_min < fwhm_dict[ptup]) and any(fwhm_dict[ptup] < fwhms_max):\n",
      "            print 'Bad parameters {} with FWHMs {}'.format(ptup, fwhm_dict[ptup])\n",
      "            if not flag:\n",
      "                flag = True\n",
      "    \n",
      "    if flag:\n",
      "        print 'WARNING: parameters give FWHMs within measurement range, fix them.'\n",
      "    else:\n",
      "        print 'All parameters give FWHMs outside measurement range, good to go'\n",
      "    \n",
      "    return fwhm_dict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "2.2 (SNR and energy binning)-specific data entry "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "2.2.1 SN 1006"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "SN 1006 min/max estimates are from Sean's paper"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "SN1006_KEVS = np.array([0.7, 1.0, 2.0])\n",
      "SN1006_MIN = np.array([23., 18., 15.])\n",
      "SN1006_MAX = np.array([49., 43., 30.])\n",
      "\n",
      "SN1006_PARS = {0.0: {1e-16: [70, 150], 1: [80, 170], 100: [180, 500], 1000: [300, 650]},\n",
      "               1.0: {1e-16: [70, 150], 1: [70, 150], 100: [150, 380], 1000: [240, 650]},\n",
      "               2.0: {1e-16: [70, 150], 1: [70, 150], 100: [110, 400], 1000: [190, 650]}}\n",
      "SN1006_PARS = rescale_B0(SN1006_PARS)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "2.2.2 Tycho's SNR"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Tycho min/max estimates are from \"regions-4\" with simple 2-exponential fit\n",
      "\n",
      "N.B. max observed FWHM at 0.7-1 keV was 6.092, but I use 10.0 to be more\n",
      "consistent with observed trend, esp. as some regions don't have a 0.7-1 FWHM."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "TYCHO_KEVS = np.array([0.7, 1.0, 2.0, 3.0, 4.5])\n",
      "TYCHO_MIN = np.array([1.628, 1.685, 1.510, 1.465, 1.370])\n",
      "TYCHO_MAX = np.array([10.0, 8.866, 6.901, 7.508, 5.763])\n",
      "\n",
      "TYCHO_PARS = {0.0: {1e-16: [115, 700], 1: [150, 700], 100: [350, 1900], 1000: [500, 2600]},\n",
      "              1.0: {1e-16: [115, 700], 1: [140, 650], 100: [300, 1500], 1000: [500, 2500]},\n",
      "              2.0: {1e-16: [115, 700], 1: [130, 600], 100: [270, 1100], 1000: [400, 2200]}}\n",
      "TYCHO_PARS = rescale_B0(TYCHO_PARS)\n",
      "\n",
      "TYCHO_KWARGS = {}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "2.2.3 Tycho's SNR with magnetic damping ($\\mu=1$ only, for simplicity)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It's very hard to get large FWHMs in magnetic damping model.\n",
      "When B is too small, the filament smears out and the FWHM becomes unmeasurable\n",
      "(background behind filament rises above half-max intensity).  So I can't always find parameters\n",
      "that give FWHMs outside of our measured range of FWHMs.\n",
      "\n",
      "Thus, ignore warnings about having to fix bad parameters mu/eta2/B0."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "TYCHO_DAMP = {}\n",
      "TYCHO_DAMP[0.5] = {1.0: {1e-16: [120, 650],\n",
      "                           0.1: [120, 650],\n",
      "                             1: [140, 650],\n",
      "                            10: [220, 800]}}  # ab = 0.5\n",
      "TYCHO_DAMP[0.01] = {1.0: {1e-16: [55, 700],\n",
      "                           0.1: [40, 650],\n",
      "                             1: [25, 600],\n",
      "                            10: [18, 900]}}  # ab = 0.01\n",
      "TYCHO_DAMP[0.005] = {1.0: {1e-16: [45, 700],\n",
      "                           0.1: [33, 600],\n",
      "                             1: [26, 600],\n",
      "                            10: [20, 550]}}  # ab = 0.005\n",
      "\n",
      "for _ab, _pars_damp in TYCHO_DAMP.items():\n",
      "    TYCHO_DAMP[_ab] = rescale_B0(_pars_damp)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "3. Configuration cell - set SNR, energy bands, parameters to use here (!)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Change these variables to run analysis with different SNRs, bounds, default grid settings, et cetera"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "snr = snrcat.make_SN1006()\n",
      "kevs = SN1006_KEVS\n",
      "fwhms_min = SN1006_MIN\n",
      "fwhms_max = SN1006_MAX\n",
      "pars = SN1006_PARS\n",
      "base_kws = {}\n",
      "\n",
      "# For damping\n",
      "#ab = 0.01\n",
      "#pars = TYCHO_DAMP[ab]\n",
      "#base_kws = {'idamp': True, 'damp_ab': ab}\n",
      "\n",
      "f = Fitter(snr, kevs, np.ones(len(kevs)), np.ones(len(kevs)),\n",
      "           {}, inds=None, verbose=True)  # Data, eps, tab not used\n",
      "\n",
      "print 'SNR ({}) settings'.format(f.snr.name)\n",
      "print '\\t','\\n\\t'.join(\"%s: %s\" % item for item in vars(f.snr).items())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Save the initial settings and allow subsequent code to reset the `Fitter()`'s `SupernovaRemnant()` object.<br />\n",
      "Then, compute and save FWHMs for the initial settings and verify that they fall within parameter bounds."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "base_fwhms = init_par_bounds(f, base_kws, pars, fwhms_min, fwhms_max)  # Dict of {(mu,eta2,B0): [fwhms], ...}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "4. Resolution analysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Functions (loosely speaking) to:\n",
      "1. compute new FWHMs in response to a change in some `Fitter()` parameter\n",
      "2. characterize the resulting fractional change in model output, i.e. our fractional error"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def recalculate(fobj, base_fwhms, base_kws, **kwargs):\n",
      "    \"\"\"Compute FWHMs from given fitter object, base_fwhms mu/eta2/B0, and kwargs\"\"\"\n",
      "    \n",
      "    kwdict = {}\n",
      "    kwdict.update(base_kws)  # Being careful not to modify original base_kws\n",
      "    kwdict.update(kwargs)\n",
      "    \n",
      "    fwhms = lview.map_async(lambda pt: fobj.width_full(*pt, **kwdict), base_fwhms)  # maps base_fwhms.keys()\n",
      "    parutils.stdout_when_done(fwhms)\n",
      "    print 'Time elapsed: {}'.format(fwhms.elapsed)\n",
      "    \n",
      "    return dict(zip(base_fwhms.keys(), fwhms))\n",
      "\n",
      "def check_frac_change(base_fwhms, mod_fwhms, f=0.001):\n",
      "    \"\"\"Compute maximum fractional change in parameter space;\n",
      "    report points w/ fractional change above some threshold\n",
      "    \"\"\"\n",
      "    fracs_all = []\n",
      "    for pt in base_fwhms:\n",
      "        arr1 = base_fwhms[pt]\n",
      "        arr2 = mod_fwhms[pt]\n",
      "        \n",
      "        frac = np.abs(arr1 - arr2) / np.minimum(arr1, arr2)  # Being conservative\n",
      "        if any(frac > f):\n",
      "            print '\\tFrac change:', ', '.join(['{:0.2g}'.format(x) for x in frac])\n",
      "            print '\\t\\tmu = {:0.2f}, eta2 = {:0.3e}, B0 = {:0.3e}'.format(*pt)\n",
      "        fracs_all.append(frac)\n",
      "    \n",
      "    fracs_all = np.array(fracs_all)\n",
      "    \n",
      "    np.set_printoptions(precision=8)\n",
      "    print 'Maximum frac. change:', np.amax(fracs_all)\n",
      "    print 'Median frac. change in each energy band: ', np.median(fracs_all, axis=0)\n",
      "    print 'Mean frac. change in each energy band: ', np.mean(fracs_all, axis=0)\n",
      "    print 'Maximum frac. change in each energy band: ', np.amax(fracs_all, axis=0)\n",
      "    return fracs_all  # For subsequent analysis, if so desired"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "4.1 Procedure (how to use this notebook)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* go back up and make sure you set the correct SNR/numbers to analyze\n",
      "* run the \"recalculation\" cells one by one\n",
      "* vary the fractional error threshold in the subsequent cells (if so desired)\n",
      "* record results in the markdown (conclusions) cell\n",
      "\n",
      "Old results might have to be overwritten, as we keep changing model settings/knobs\n",
      "\n",
      "As you go about running cells nilly-willy, be sure that the output results of each cell\n",
      "were run immediately after the calculation step, since I am recycling the same variable names."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "4.2 `iradmax` -- resolution for tabulated e- distribution $f(E,r)$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is probably the most important grid resolution: e- distribution calculation is both time intensive and (hence) least precise.\n",
      "Roughly speaking, this is interchangeable with `rminarc`.  To maintain precision, you should vary them together -- doubling `rminarc` requires doubling of `iradmax` resolution (neglecting smaller effects)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mod_fwhms = recalculate(f, base_fwhms, base_kws, iradmax=200)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "frac_err = check_frac_change(base_fwhms, mod_fwhms, f=0.0005)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "mod_fwhms = recalculate(f, base_fwhms, base_kws, iradmax=50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "frac_err = check_frac_change(base_fwhms, mod_fwhms, f=0.0005)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notes (2014 October 10)\n",
      "\n",
      "__Tycho__: default `iradmax = 100` with default adaptive `rminarc` calculation looks good.<br>\n",
      "Maximum error from halving is 0.5% (mean/median ~ 0.02 to 0.1%).<br>\n",
      "Maximum error from doubling is 0.3% (mean/median ~ 0.003% to 0.06%)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "4.3 `ixmax` -- resolution on line-of-sight integration"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mod_fwhms = recalculate(f, base_fwhms, base_kws, ixmax=1000)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "frac_err = check_frac_change(base_fwhms, mod_fwhms, f=0.001)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mod_fwhms = recalculate(f, base_fwhms, base_kws, ixmax=250)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "frac_err = check_frac_change(base_fwhms, mod_fwhms, f=0.001)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "4.4 `irmax` -- resolution on intensity profile (should not matter)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mod_fwhms = recalculate(f, base_fwhms, base_kws, irmax=400)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "frac_err = check_frac_change(base_fwhms, mod_fwhms, f=0.0001)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mod_fwhms = recalculate(f, base_fwhms, base_kws, irmax=100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "frac_err = check_frac_change(base_fwhms, mod_fwhms, f=0.0001)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "4.5 `irhomax` -- resolution on emissivity grid (should not matter)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As long as `irhomax > iradmax`, I expect that it should not matter because the emissivity grid is simpliy interpolating e- distribution, and intensity integrals will simply interpolate the emissivity grid!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "mod_fwhms = recalculate(f, base_fwhms, base_kws, irhomax=4000)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "frac_err = check_frac_change(base_fwhms, mod_fwhms, f=0.0001)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "mod_fwhms = recalculate(f, base_fwhms, base_kws, irhomax=1000)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "frac_err = check_frac_change(base_fwhms, mod_fwhms, f=0.0001)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "4.5 Table of 1-particle synchrotron emissivity (originally from Pacholczyk)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Table for single particle emissivity originally had 35 entries for scaled single particle synchrotron emissivity numbers (which are just some funkily integrated Bessel function values).  Now using a new table with 89 scipy computed entries.\n",
      "\n",
      "Investigate a few distinct effects:\n",
      "1. use scipy computed values for integrated Bessel functs<br>\n",
      "   (Pacholczyk is almost 50 years old, and tabulated numbers have errors 1-5% at worst)<br>\n",
      "   (this turned out to have very little effect)\n",
      "2. sample table entries evenly in log-space, instead of using hand-picked entries\n",
      "   (choosing by hand allows us to get more point near areas of curvature, etc, but we don't know if it's good or not)\n",
      "3. denser table sampling\n",
      "4. pull in additional values at the tails of the spectrum (e.g., extend down to 1e-6, extend up to 1e3)\n",
      "\n",
      "You're responsible for cleaning up the generated `fglists-test.dat` file when done."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy as sp\n",
      "from scipy import integrate\n",
      "from scipy import special\n",
      "\n",
      "def F(x):\n",
      "    res = sp.integrate.quad(lambda z: sp.special.kv(5./3., z), x, np.inf)\n",
      "    return x * res[0]\n",
      "\n",
      "def make_fglists_test(xext):\n",
      "    fext = np.array(map(F, xext))\n",
      "    xext = np.append(xext, 0)\n",
      "    fext = np.append(fext, 0)\n",
      "    np.savetxt('fglists-test.dat', np.array([xext,fext]).T)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is the current list of xex values (89 total) being used -- use this as a reference to generate new tables for testing."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "xex = [np.logspace(-4, -3, 4, base=10, endpoint=False),\n",
      "       np.logspace(-3, -2, 4, base=10, endpoint=False),\n",
      "       np.logspace(-2, -1, 8, base=10, endpoint=False),\n",
      "       np.logspace(-1, 0, 24, base=10, endpoint=False),  # Peak at x ~ 0.3\n",
      "       np.logspace(0, 1, 24, base=10, endpoint=False),  # Get spectrum curvature\n",
      "       np.logspace(1, 2, 25, base=10)]  # Low e- electrons may contribute\n",
      "xex = np.concatenate(xex)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "4.6.1 Test even log-spaced sampling and doubled/halved table sampling"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Start by varying Sean's provided table w/ 35 entries, drawn from Pacholczyk"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# Evenly log-spaced sampling\n",
      "xext = np.logspace(-4,2,89,base=10)\n",
      "\n",
      "make_fglists_test(xext)\n",
      "mod_fwhms = recalculate(f, base_fwhms, base_kws, fgfname='fglists-test.dat')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "frac_err = check_frac_change(base_fwhms, mod_fwhms, f=0.001)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test doubled resolution\n",
      "xext = [np.logspace(-4, -3, 8, base=10, endpoint=False),\n",
      "       np.logspace(-3, -2, 8, base=10, endpoint=False),\n",
      "       np.logspace(-2, -1, 16, base=10, endpoint=False),\n",
      "       np.logspace(-1, 0, 48, base=10, endpoint=False),\n",
      "       np.logspace(0, 1, 48, base=10, endpoint=False),\n",
      "       np.logspace(1, 2, 49, base=10)]\n",
      "xext = np.concatenate(xext)\n",
      "\n",
      "make_fglists_test(xext)\n",
      "mod_fwhms = recalculate(f, base_fwhms, base_kws, fgfname='fglists-test.dat')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "frac_err = check_frac_change(base_fwhms, mod_fwhms, f=0.001)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test halved resolution\n",
      "xext = [np.logspace(-4, -3, 2, base=10, endpoint=False),\n",
      "       np.logspace(-3, -2, 2, base=10, endpoint=False),\n",
      "       np.logspace(-2, -1, 4, base=10, endpoint=False),\n",
      "       np.logspace(-1, 0, 12, base=10, endpoint=False),\n",
      "       np.logspace(0, 1, 12, base=10, endpoint=False),\n",
      "       np.logspace(1, 2, 13, base=10)]\n",
      "xext = np.concatenate(xext)\n",
      "\n",
      "make_fglists_test(xext)\n",
      "mod_fwhms = recalculate(f, base_fwhms, base_kws, fgfname='fglists-test.dat')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "frac_err = check_frac_change(base_fwhms, mod_fwhms, f=0.001)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "4.6.2 Add more points on each side of 1-particle synchrotron spectrum"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Append more low frequency (high e- energy) samples\n",
      "xext = np.append(np.logspace(-6, -4, 8, base=10, endpoint=False), xex)\n",
      "\n",
      "make_fglists_test(xext)\n",
      "mod_fwhms = recalculate(f, base_fwhms, base_kws, fgfname='fglists-test.dat')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "frac_err = check_frac_change(base_fwhms, mod_fwhms, f=0.001)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Append more high frequency (low e- energy) samples\n",
      "xext = np.append(xex[:-1], np.logspace(2, 3, 9, base=10))  # Just appending 1e2 to 1e3\n",
      "\n",
      "make_fglists_test(xext)\n",
      "mod_fwhms = recalculate(f, base_fwhms, base_kws, fgfname='fglists-test.dat')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "frac_err = check_frac_change(base_fwhms, mod_fwhms, f=0.001)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "4.7 Internal e- distribution integrals"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Integrals in Fortran e- distribution code\n",
      "* e- distribution ($\\mu<1$): integral over $t$ with 1000 pts (default)\n",
      "* e- distribution ($\\mu=1$): integral over $t$ w/ 1000 pts (5000 for damping??), $n$ w/ 100 pts\n",
      "* e- distribution ($\\mu>1$): integral over $t$ w/ 1000 pts, $n$ w/ 100 pts\n",
      "Let's group these into \"$t$\" integrals and \"$n$\" integrals...\n",
      "\n",
      "Integrals in Python port\n",
      "* emistab resolution (irhomax), this shouldn't really affect much.  Large irhomax just interpolates a smaller iradmax and eases later computation"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "4.7.0 All mu values, halve/double integral resolutions across the board"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mod_fwhms = recalculate(f, base_fwhms, base_kws, itmax=400, inmax=100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "frac_err = check_frac_change(base_fwhms,mod_fwhms, f=0.001)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mod_fwhms = recalculate(f, base_fwhms, base_kws, itmax=100, inmax=25)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "frac_err = check_frac_change(base_fwhms, mod_fwhms, f=0.0001)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "5. Conclusions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Remember that, because we just sample very few points, these statements of error are not all that rigorous.\n",
      "Basically the same as Sean's estimation, probably, except I have it codified is all.\n",
      "\n",
      "Some degree of variation in reported errors, esp. for magnetic damping (because parameters being inspected are different for each scale length $a_b$, causes reported errors to fluctuate quite a bit too.  And maximum errors are, of course, controlled by just one corner of parameter space."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "5.1 Tycho"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "2014 October 10\n",
      "\n",
      "__Default, loss limited settings__\n",
      "\n",
      "Gridding, integral resolutions\n",
      "\n",
      "* iradmax max error is ~0.5% (mean/median ~0.02 to 0.1%) (from halving)\n",
      "* ixmax max error ~0.007% (so, 0.01%), mean/median 0.001% ish. (from halving)\n",
      "* irmax max change is ~0.000001%, mean/median ~1e-11 to 1e-10 (1e-9 to 1e-8 %)<br>\n",
      "  default `irmax=200`. largest change of 1e-8 occurs if `irmax` doubled to 400.<br>\n",
      "  Halving `irmax` gives a smaller change, since we are relying mainly on scipy rootfinders at this point.\n",
      "* irhomax max error ~0.07% (mean/median ~ 0.0001% to 0.01%, max error at high energy)<br>\n",
      "  halving irhomax (2000 to 1000) gives max err: 0.07% or 7e-4 (median/mean ~1e-6 to 1e-4)\n",
      "* halving itmax/inmax: max error 0.02%, median ~0.001%, mean ~0.003%.\n",
      "\n",
      "e- emissivity table (fglists)\n",
      "\n",
      "* Even log sampling, change in values: 0.4% max (median 0.0001%, mean 0.01-0.1%).<br>\n",
      "  Biggest error consistently at eta2 = 1e-16, advective solution.\n",
      "* 2x resolution err at most 0.2% (median 0.0001%, mean ~ 0.01 to 0.04%)\n",
      "* 1/2 resolution error at most 0.4% (median 0.0001 to 0.001%, mean 0.02 to 0.06%)\n",
      "* Add more low-freq samples: max err 0.008% (median err ~1e-10, mean ~ 1e-6 to 1e-5)\n",
      "* Add more high-freq samples: max err 0.0000001%\n",
      "\n",
      "So, the biggest error from any knob individually is about 0.5% (`iradmax`, `fglists` sampling).<br>\n",
      "The mean/median errors we expect should be around 0.01 to 0.1%.\n",
      "\n",
      "__With very weak damping ($a_b = 0.5$):__\n",
      "\n",
      "* iradmax, max err 0.5%, median 0.03-0.04%, mean 0.07-0.1% (halving)\n",
      "* ixmax, max err 0.004%\n",
      "* irmax, fine\n",
      "* irhomax, max err 0.04% (halving)\n",
      "* itmax/inmax, max err from halving is 0.008% (0.01% roughly)\n",
      "\n",
      "\n",
      "* e- distr, log samp, max err 0.6%\n",
      "* e- distr, half/double resolution, max errs 0.3% and 0.4% each way.\n",
      "* e- distr, add more pts on left/right: max err (left) 0.003%\n",
      "\n",
      "__With moderate damping ($a_b = 0.01$):__\n",
      "\n",
      "* iradmax: doubling max err 0.3%, halving max err 1% (!!!)<br>\n",
      "  The next largest error is 0.5%.\n",
      "* ixmax: max err (halving) is 0.02%\n",
      "* irmax: fine\n",
      "* irhomax: max error (halving) is 0.1%\n",
      "* itmax/inmax double: 0.08%\n",
      "* itmax/inmax half: 1.4% max err (median 0.003%, mean 0.1-0.2%)\n",
      "\n",
      "\n",
      "* e- distr log samp: 0.4% change\n",
      "* e- distr 2x samp: 0.2%\n",
      "* e- distr 1/2 samp: 0.4%\n",
      "* more pts left/right e- distr: fine\n",
      "\n",
      "\n",
      "__With very strong damping ($a_b = 0.005$):__\n",
      "\n",
      "* iradmax:    halving, max err 0.6% (median 0.04 to 0.1%, mean 0.05 to 0.2%)\n",
      "* ixmax:      halving, max err 0.01%\n",
      "* irmax:      fine (1e-7 %)\n",
      "* irhomax:    halving, 0.04% err\n",
      "* itmax/inmax: halving, 0.5% max err (!), median 0.002%, mean 0.04-0.1%<br>\n",
      "  doubling, 0.2% max, median/mean smaller\n",
      "\n",
      "\n",
      "* e- distr log samp:   max err 0.1%, median <0.004%, mean <0.02%\n",
      "* e- distr 2x res:     max err 0.2%, median <0.002%, mean <0.03%\n",
      "* e- distr 1/2 res:    max err 0.5%, median <0.01%, mean <0.1%\n",
      "* e- distr lo freq pt: max err 0.01%\n",
      "* e- distr hi freq pt: max err 0.00000001%"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "5.2 SN 1006"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "__Loss limited case only__\n",
      "\n",
      "SN 1006\n",
      "\n",
      "* iradmax: halving max err 0.4% (median 0.02%, mean 0.05-0.1%)\n",
      "* ixmax: halving err 0.005%, doubling 0.001%\n",
      "* irmax: fine (0.000001% max)\n",
      "* irhomax: fine (0.001% max, halving)\n",
      "* inmax/itmax: halving err 0.02% (fine)\n",
      "\n",
      "\n",
      "* e- distr, log samp: 0.45% max, median ~ 0.0001%, mean 0.02-0.06%\n",
      "* e- distr, 2x res: 0.3% max\n",
      "* e- distr, 1/2 res: 0.3% max (median ~ 0.0005%, mean ~ 0.01-0.06%)\n",
      "* e- distr, add pts lo/hi: fine"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}