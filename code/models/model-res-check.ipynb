{
 "metadata": {
  "name": "",
  "signature": "sha256:1d3f78ba911fa2c2f8a89bcc72944e47eab34cb7c929d24254889c07902c3c9c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Check full model code resolution errors, all SNRs"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our numerical full model code carries a resolution error, which approaches zero as our resolution is increased (i.e., our discrete code converges towards an exact solution [granted, considering each discretization/mesh separately]).  But, without an analytic solution, we can't compute the error directly (if we had the analytic solution we'd just use that, anyways).\n",
      "\n",
      "We estimate resolution error un-rigorously by computing fractional error, increasing resolution until fractional error is smaller than our tolerance, and praying that the absolute error (sum of fractional errors) will be small enough at the end.\n",
      "\n",
      "It is worth thinking about this more deeply, but it's unfortunately a low priority right now. and our model is a bit of a black box.  I imagine we could look at the integrals and compute error bounds on each one analytically, then propagate the errors through somehow.  Might be good as an exercise.\n",
      "\n",
      "Caveat: we are only looking at fractional error in FWHMs, we have not considered fractional error in best-fit parameters ($B_0$, $\\eta_2$)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "1. Imports, set-up fitters for computation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import division\n",
      "\n",
      "from IPython import parallel\n",
      "\n",
      "import cPickle as pickle\n",
      "import numpy as np\n",
      "\n",
      "import models\n",
      "import models_exec as mex\n",
      "from models_exec import Fitter\n",
      "import parutils\n",
      "import snr_catalog as snrcat"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Parallelize to speed up computation (remember to start the cluster first)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lview = parutils.get_lview()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Utilities to create `Fitter()` objects and manipulate SNR settings being investigated"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def init_fitter(snr, kevs):\n",
      "    \"\"\"Generate fitter for some SNR.  Data, eps, tab don't matter\"\"\"\n",
      "    data, eps = np.ones(len(kevs)), np.ones(len(kevs))\n",
      "    tab = {}\n",
      "    return Fitter(snr, kevs, data, eps,\n",
      "                  tab, inds=None, verbose=True)\n",
      "\n",
      "def get_snrvars(fobj):\n",
      "    print 'SNR ({}) settings'.format(fobj.snr.name)\n",
      "    print '\\t','\\n\\t'.join(\"%s: %s\" % item for item in vars(fobj.snr).items())\n",
      "    return vars(fobj.snr).items()\n",
      "\n",
      "def set_snrvars(fobj, vdict):\n",
      "    for par, val in vdict:\n",
      "        vars(fobj.snr)[par] = val\n",
      "    return fobj"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "2. Assemble data and parameters to check"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Bounding parameters are determined by trial and error to give FWHMs outside of FWHM bounds.\n",
      "Simply use a `Fitter()` object's `width_full` method (merely a wrapper for `models.width_full`).\n",
      "\n",
      "Parameters are akin to tables: dicts keyed by mu, then eta2, then a list of min/max B0 values.\n",
      "I input B0 values in $\\mu\\mathrm{G}$ and then rescale, for clarity/convenience."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "2.1 Utilities for parameter space points $(\\mu$, $\\eta_2$, $B_0)$ being investigated"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def rescale_B0(p):\n",
      "    for mu in p:\n",
      "        for eta2 in p[mu]:\n",
      "            p[mu][eta2] = 1e-6 * np.array(p[mu][eta2])\n",
      "    return p\n",
      "\n",
      "def init_par_bounds(fobj, pars, fwhms_min, fwhms_max):\n",
      "    \"\"\"Check that parameter bounds are valid, return dict of FWHMs\n",
      "    Could be parallelized, but probably not worth the time\n",
      "    (in principle, method only has to be run 1x/SNR... maybe 1-10x in practice)\n",
      "    \"\"\"\n",
      "    print 'Checking for valid parameter bounds'\n",
      "    fwhm_dict = {}\n",
      "    \n",
      "    for mu in pars:\n",
      "        for eta2 in pars[mu]:\n",
      "            B_min, B_max = pars[mu][eta2]\n",
      "            fwhms_hi = fobj.width_full(mu, eta2, B_min)  # Minimum B0, max FWHMs\n",
      "            fwhms_lo = fobj.width_full(mu, eta2, B_max)  # Maximum B0, min FWHMs\n",
      "            if any(fwhms_lo > fwhms_min) or any(fwhms_hi < fwhms_max):\n",
      "                raise ValueError('Bad parameters mu={}, eta2={}, B0 range [{}, {}]'.format(mu, eta2, B_min, B_max))\n",
      "            else:\n",
      "                fwhm_dict[(mu, eta2, B_min)] = fwhms_hi\n",
      "                fwhm_dict[(mu, eta2, B_max)] = fwhms_lo\n",
      "    \n",
      "    print 'All parameters give FWHMs outside measurement range, good to go'\n",
      "    return fwhm_dict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "2.2 (SNR and energy binning)-specific data entry "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "2.2.1 SN 1006"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "SN 1006 min/max estimates are from Sean's paper"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "SN1006_KEVS = np.array([0.7, 1.0, 2.0])\n",
      "SN1006_MIN = np.array([23., 18., 15.])\n",
      "SN1006_MAX = np.array([49., 43., 30.])\n",
      "\n",
      "SN1006_PARS = {0.0: {1e-16: [70, 180], 100: [180, 500], 1000: [230, 650]},\n",
      "               1.0: {1e-16: [70, 180], 100: [140, 380], 1000: [240, 650]},\n",
      "               2.0: {1e-16: [70, 180], 100: [110, 400], 1000: [190, 650]}}\n",
      "SN1006_PARS = rescale_B0(SN1006_PARS)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "2.2.2 Tycho's SNR"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Tycho min/max estimates are from \"regions-4\" with simple 2-exponential fit\n",
      "\n",
      "N.B. max observed FWHM at 0.7-1 keV was 6.092, but I use 10.0 to be more\n",
      "consistent with observed trend, esp. as some regions don't have a 0.7-1 FWHM."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "TYCHO_KEVS = np.array([0.7, 1.0, 2.0, 3.0, 4.5])\n",
      "TYCHO_MIN = np.array([1.628, 1.685, 1.510, 1.465, 1.370])\n",
      "TYCHO_MAX = np.array([10.0, 8.866, 6.901, 7.508, 5.763])\n",
      "\n",
      "TYCHO_PARS = {0.0: {1e-16: [115, 700], 100: [350, 1900], 1000: [380, 2400]},\n",
      "              1.0: {1e-16: [115, 700], 100: [300, 1500], 1000: [500, 2500]},\n",
      "              2.0: {1e-16: [115, 700], 100: [250, 1200], 1000: [400, 2200]}}\n",
      "TYCHO_PARS = rescale_B0(TYCHO_PARS)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "3. Configuration cell - set SNR, energy bands, parameters to use here (!)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Change these variables to run analysis with different SNRs, bounds, default grid settings, et cetera"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "snr = snrcat.make_SN1006()\n",
      "kevs = SN1006_KEVS\n",
      "fwhms_min = SN1006_MIN\n",
      "fwhms_max = SN1006_MAX\n",
      "pars = SN1006_PARS\n",
      "f = init_fitter(snr, kevs)\n",
      "# Modify fitter settings, e.g. SNR compression ratio etc., here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Save the initial settings and allow subsequent code to reset the `Fitter()`'s `SupernovaRemnant()` object.<br />\n",
      "Then, compute and save FWHMs for the initial settings and verify that they fall within parameter bounds."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "init_f = get_snrvars(f)\n",
      "def reset_fitter(fobj):\n",
      "    set_snrvars(fobj, init_f)  # If I'm gonna use globals, may as well take full advantage of them"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "base_fwhms = init_par_bounds(f, pars, fwhms_min, fwhms_max)  # Dict of {(mu,eta2,B0): [fwhms], ...}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "4. Resolution analysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Functions (loosely speaking) to:\n",
      "1. compute new FWHMs in response to a change in some `Fitter()` parameter\n",
      "2. characterize the resulting fractional change in model output, i.e. our fractional error"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def make_fmod(vname, fappl):\n",
      "    \"\"\"Factory function, to make functions which apply operations to Fitter().snr properties\"\"\"\n",
      "    def f_appl(fobj):\n",
      "        vval = vars(fobj.snr)[vname]\n",
      "        vars(fobj.snr)[vname] = fappl(vval)\n",
      "        print 'Initial {} = {}'.format(vname, vval)\n",
      "        print 'Final {} = {}'.format(vname, vars(fobj.snr)[vname])\n",
      "        return fobj\n",
      "    return f_appl\n",
      "\n",
      "def recalculate(fobj, fmod, base_fwhms):\n",
      "    \"\"\"Compute new FWHMs by applying fmod to fobj.  Reset fobj when done.\"\"\"\n",
      "    fobj = fmod(fobj)\n",
      "    fwhms = lview.map_async(lambda pt: fobj.width_full(*pt), base_fwhms)  # maps over keys\n",
      "    parutils.stdout_when_done(fwhms)\n",
      "    print 'Time elapsed: {}'.format(fwhms.elapsed)\n",
      "    reset_fitter(fobj)\n",
      "    return dict(zip(base_fwhms.keys(), fwhms))\n",
      "\n",
      "def check_frac_change(base_fwhms, mod_fwhms, f=0.001):\n",
      "    \"\"\"Compute maximum fractional change in parameter space;\n",
      "    report points w/ fractional change above some threshold\n",
      "    \"\"\"\n",
      "    fracs_all = []\n",
      "    for pt in base_fwhms:\n",
      "        arr1 = base_fwhms[pt]\n",
      "        arr2 = mod_fwhms[pt]\n",
      "        \n",
      "        frac = np.abs(arr1 - arr2) / np.minimum(arr1, arr2)  # Being conservative\n",
      "        if any(frac > f):\n",
      "            print '\\tFrac change:', ', '.join(['{:0.2g}'.format(x) for x in frac])\n",
      "            print '\\t\\tmu = {:0.2f}, eta2 = {:0.3e}, B0 = {:0.3e}'.format(*pt)\n",
      "        fracs_all.append(frac)\n",
      "    \n",
      "    fracs_all = np.array(fracs_all)\n",
      "    \n",
      "    np.set_printoptions(precision=8)\n",
      "    print 'Maximum frac. change:', np.amax(fracs_all)\n",
      "    print 'Median frac. change in each energy band: ', np.median(fracs_all, axis=0)\n",
      "    print 'Mean frac. change in each energy band: ', np.mean(fracs_all, axis=0)\n",
      "    print 'Maximum frac. change in each energy band: ', np.amax(fracs_all, axis=0)\n",
      "    return fracs_all  # For subsequent analysis, if so desired"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "4.1 Procedure (how to use this notebook)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* go back up and make sure you set the correct SNR/numbers to analyze\n",
      "* run the \"recalculation\" cells one by one\n",
      "* vary the fractional error threshold in the subsequent cells (if so desired)\n",
      "* record results in the markdown (conclusions) cell; this should be append-only, in principle\n",
      "\n",
      "As you go about running cells nilly-willy, be sure that the output results of each cell\n",
      "were run immediately after the calculation step, since I am recycling the same variable names."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "4.2 `iradmax` -- resolution for tabulated e- distribution $f(E,r)$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is probably the most important grid resolution: e- distribution calculation is both time intensive and (hence) least precise.\n",
      "Roughly speaking, this is interchangeable with `rminarc`.  To maintain precision, you should vary them together -- doubling `rminarc` requires doubling of `iradmax` resolution (neglecting smaller effects)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "4.2.1 Doubling iradmax"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f_mod = make_fmod('iradmax', lambda x: 2*x)\n",
      "mod_fwhms = recalculate(f, f_mod, base_fwhms)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "frac_err = check_frac_change(base_fwhms, mod_fwhms, f=0.0005)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "4.2.2 Halving iradmax"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f_mod = make_fmod('iradmax', lambda x: int(x/2.))\n",
      "mod_fwhms = recalculate(f, f_mod, base_fwhms)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "frac_err = check_frac_change(base_fwhms, mod_fwhms, f=0.0005)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "4.2.3 `iradmax` conclusions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notes (2014 August 29)\n",
      "\n",
      "* __SN 1006__: default `iradmax = 100` (inherited from Sean) looks acceptable, error is perhaps ~0.5% or so.<br>\n",
      "  Halving (100 to 50) error at most 1.2% (median, mean around 0.3--0.5%);<br>\n",
      "  doubling (100 to 200) error at most 0.21% (median, mean at most 0.08%).\n",
      "\n",
      "Some tests on Tycho forced my hand and I went ahead and implemented adaptive `rminarc` calculation.\n",
      "\n",
      "* __Tycho__: default `iradmax = 100` (with `rminarc=20`) is kind of abysmal.<br>\n",
      "  Halving (100 to 50), error is up to 33% (!), though 1-10% is more typical<br>\n",
      "  Doubling (100 to 200), error is up to 8%, more typically 0.1-2%\n",
      "* __Tycho__: default `iradmax = 100` (with `rminarc = [20., 16.5, 13.5, 14., 14.]`) not much better<br>\n",
      "  Halving (100 to 50), error is up to 14%, typical values are 2-3% (mean), 0.1-0.2% (median), most extreme 8% in lower bands<br>\n",
      "  Doubling (100 to 200), error is at most 7%, typical values are 0.4-2% (mean), 0.04-0.1% (median), most extreme 1-4% in lower bands\n",
      "* __Tycho__: default `iradmax = 200` (with `rminarc = [20., 16.5, 13.5, 14., 14.]`)<br>\n",
      "  Halving (100 to 200) was done above<br>\n",
      "  Doubling (200 to 400), max error is 1.6%, typical errors 0.01-0.06% (median), 0.05-0.40% (mean), most extreme 0.1-1% in lower bands\n",
      "\n",
      "As expected, most extreme errors occur at small eta2, large B0, highest energy.  On flip side, largest errors at low energies require large eta2, small B0; effect of mu is equivocal (could investigate but not so important).\n",
      "\n",
      "After implementing adaptive `rminarc` calculation:\n",
      "* __Tycho__: default `iradmax = 100` (with `rminarc=20`, adaptive `rminarc`, `irad_adapt_f=1.2`) -- beautiful!<br>\n",
      "  Halving (100 to 50), max error is 0.6%, typical errors are 0.005%-0.01% (median), 0.02-0.07% (mean), extremes 0.06-0.6%.<br>\n",
      "  Only 3 points have errors above 0.1%, all in case eta2 = 1e-16, B0 = 700 muG (thin filaments, no diffusion).<br>\n",
      "  Only 6 points have errors above 0.05%, all in case eta2 = 1e-16 (now incl. widest filaments, no diffusion).<br>\n",
      "  Doubling (100 to 200), max error is 0.13%, typical errors are 0.001-0.002% (median), 0.01-0.03% (mean); extremes 0.05-0.1%.<br>\n",
      "  Again only 3 points above 0.1% error, and 6 points above 0.05% error\n",
      "* __SN1006__: default `iradmax = 100` (with `rminarc=60`, adaptive `rminarc`, `irad_adapt_f=1.2`)<br>\n",
      "  Halving (100 to 50) max error 0.5%, typical errors 0.005-0.007% (median), 0.02-0.07% (mean), extremes 0.1-0.5%<br>\n",
      "  Doubling (100 to 200) max error 0.10%, typical errors 0.001-0.002% (median), 0.005-0.02% (mean), extreme values 0.02-0.1%\n",
      "\n",
      "Maybe not unexpectedly, errors for SN 1006 didn't drop as much (and they weren't so bad to begin with).\n",
      "Having adaptive `rminarc` brought them closer together, which makes sense and is a good sign.\n",
      "\n",
      "Curious observation -- why are all the large errors at zero diffusion?  Could this be related to the advective solution that we are calculating?  Well, it's okay, we've shown it's a ~1% effect.  So who cares."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "4.3 `ixmax` -- resolution on line-of-sight integration"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "4.3.1 Double `ixmax`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f_mod = make_fmod('ixmax', lambda x: 2*x)\n",
      "mod_fwhms = recalculate(f, f_mod, base_fwhms)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "frac_err = check_frac_change(base_fwhms, mod_fwhms, f=0.001)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "4.3.2 Halve `ixmax`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f_mod = make_fmod('ixmax', lambda x: int(x/2.))\n",
      "mod_fwhms = recalculate(f, f_mod, base_fwhms)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "frac_err = check_frac_change(base_fwhms, mod_fwhms, f=0.001)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "4.3.3 `ixmax` conclusions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(August 29, 2014 -- prior to adding `iradadapt` option, which should only decrease errors)\n",
      "\n",
      "* __SN1006:__ default of 500 is more than sufficient, error is perhaps 0.005%.<br>\n",
      "  Doubling (500 to 1000) had error at most 0.001%, typical error 0.0004%.<br>\n",
      "  Halving (500 to 250) had error at most 0.01%, typical error 0.002%.\n",
      "* __Tycho:__ default of 500 is okay.  Error like above...<br>\n",
      "  Doubling (500 to 1000) had error at most 0.001%, typical error ~0.0003%<br>\n",
      "  Halving (500 to 250) had error at most 0.01%, typical error ~ 0.003%.\n",
      "\n",
      "In Tycho, the worst error is at low energy -- makes sense, since larger FWHMs require a longer line-of-sight integration"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "4.4 `irmax` -- expectation is that this should not matter!"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "4.4.1 Double `irmax`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f_mod = make_fmod('irmax', lambda x: x*2)\n",
      "mod_fwhms = recalculate(f, f_mod, base_fwhms)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "frac_err = check_frac_change(base_fwhms, mod_fwhms, f=0.0001)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "4.4.2 Halve `irmax`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f_mod = make_fmod('irmax', lambda x: int(x/2.))\n",
      "mod_fwhms = recalculate(f, f_mod, base_fwhms)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "frac_err = check_frac_change(base_fwhms, mod_fwhms, f=0.0001)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "4.4.3 Conclusions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(August 29, 2014, prior to adding `iradadapt` option)\n",
      "* __SN1006:__ nothing happens (error ~ 1e-10 to 1e-11 typically, absolute worst 1e-9 = 0.0000001%)\n",
      "* __Tycho:__ same.  Worst error is about 5e-9; typical error is about 1e-11 to 1e-10."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "4.5 Double Pacholczyk table resolution?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Table for single particle emissivity has (only?) 35 entries for scaled single particle synchrotron emissivity numbers (which is basically a table of some ugly Bessel function values).  Do we need to add more entries?\n",
      "\n",
      "(check out copy of Pacholczyk again from Goddard library)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "4.6 Update Pacholczyk table numbers (new Bessel function values)?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Do we need to put in more accurate table numbers?  I checked Pacholczyk's values against Wolfram|Alpha at some point and they disagreed slightly (probably order 1-10% at most).  What happens if we do this?"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}