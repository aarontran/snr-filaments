Research notes
==============

Aaron Tran
Summer 2014

This is a running log of thoughts, actions -- a lazy man's lab notebook.

Table of contents?
==================

* Week 1 - DS9 region selection and first look at profiles
* Week 2 - CIAO/HEASOFT setup, specextract to check thermal emission in regions
* Week 3 - iMac setup, PyXspec script, start profile fitting
* Week 4 - profile fitting expts, new energy bands
* Week 5 - fix profile fit errors, FWHM and `m_E` calculation, Sean's code
* Week 6 - TBD



(Week 1) Monday, 2014 June 2
============================

* Morning NASA orientation
* Meeting with Rob Petre until about 1-2p?
* Not much work done (I can't even remember what I did...)
* CRESST orientation


Tuesday, 2014 June 3
====================

Summary
-------

* Installed ds9 to ~/bin. In `.bash_profile`, add: `export PATH=~/bin:$PATH`
* Messed around on ds9, went through user's manual.  Played with ds9 commands.

Agenda, useful items?
---------------------

[ds9 command line options](http://ds9.si.edu/ref/command.html) will be useful
Looks like we need pyds9, or XPA, for consecutive commands...

Need to cover, for my own learning
1. basic radiative processes (thermal, nonthermal emission)
2. supernova physics (temperatures, radiation/spectra during evolution)
3. physics of Tycho -- interpretation of the fluff.  ISM gradient.
4. Ressler et al. (in press, ApJ)

5. X-ray telescopes.  Resolution, operation, limitations

whether in some kind of lab notebook, LaTeX file, or whatever

Meanwhile: construct and save shape files of the regions for which to extract
spectra.
1. Make a preliminary region, just eyeballing it / doing it by hand
2. Read Ressler.  Look at any similar papers where people look at filament
   spectra
3. Figure out if there's any more systematic way.  How far beyond the shock, or
   interior of the shock, should the region extend?  Physics motivating this.

Main Tycho (SN 1572) data
-------------------------

All files have been generated by Brian, sent to Rob.
He's already done the processing work with CIAO etc...

Three data files are:

* `0.7-1kev_mosaic.fits`
* `1-2kev_mosaic.fits`
* `2-7kev_mosaic.fits`

Resolution: 1 arcsecond (downsampled, Chandra has 0.5 arcsec resolution).
To verify this, poking around in DS9 (looking at pixel coords):
* RA:  0.98 arcsec = 0.150 seconds * (360 deg/24 hr) * cos(64.167 deg)
* DEC: ~1.0 arcsec

We also have `2009_3to7kev.fits`, resolution is 0.5 arcsec (very few counts)

Meddling with ds9
-----------------

Generating an RGB image using these images may be useful to help,
e.g., identify thermal contamination using all bands.

Scaling limits: goal is to simply reject background.
Method: look @ heatmap -- SNR should be ~pure white, background is colorful.
Mainly, we lose some information about high count pixels
(only O(10^1) pixels will thus saturate)

* 0.7-1 keV: 4e-8, 2.1e-6
* 1-2 keV:   4e-8, 2.42e-6
* 2-7 keV:   3e-8, 1.5e-6

Example "one-liner" for an RGB image (modified Jun 4):

    ds9 -rgb \
        -red 0.7-1kev_mosaic.fits \
            -scale limits 4e-8 2.1e-6 \
        -green 1-2kev_mosaic.fits \
            -scale limits 4e-8 2.42e-6 \
        -blue 2-7kev_mosaic.fits \
            -scale limits 3e-8 1.5e-6 \
        -rgb lock scale yes \
        -asinh

For a single file:

    ds9 2-7kev_mosaic.fits \
        -scale limits 4e-8 2.1e-6 -asinh -cmap Heat

Qualitatively:
* sqrt is useful to bring out thin/wispy filaments (bottom-left / SE corner)
* asinh, histogram eq. are useful for contrast, dramatic nice pictures.
  Can definitely see stripping, little wisps, esp. in 2-7 keV

sqrt makes low energy stuff slightly brighter, asinh makes high energy stuff
brighter.  In general, both serve to pull out low energy detail, and brighten
high energy stuff (closer to saturating).  Hist. eq. does this even more
extremely, it seems.

Radial profile selection with ds9
---------------------------------
Region specification:

    projection x1 y1 x2 y2 width

(x1, y1) and (x2, y2) specify coordinates of bounding box, I suppose

My next question is how do you specify a width?



Wednesday, 2014 June 4
======================

Summary
-------
* Began selecting regions for fits
* Met with Rob about next plans
* Laid out next steps (for processing fits)
* Found strange object inside Tycho

Selecting regions
-----------------

Setup command for generating projection regions:

    ds9 -rgb \
        -red 0.7-1kev_mosaic.fits \
            -scale limits 4e-8 2.1e-6 \
        -green 1-2kev_mosaic.fits \
            -scale limits 4e-8 2.42e-6 \
        -blue 2-7kev_mosaic.fits \
            -scale limits 3e-8 1.5e-6 \
        -rgb lock scale yes \
        -histequ

Note that order matters here -- lock the scale after setting limits in each
channel, so they will all have different limits.

Agenda for now
1. Get some rough guesses / shapes / regions to save, by eyeballing...
2. Figure out how to generate plots from those regions automatically.
3. Think about calculating ``radial distance''.  How do we get the center?
    a. Centroid of gamma-ray emission given by (Acciari et al., 2011).
       But, this is offset by 0.04 deg. = 2.4 arcminutes
       Compare the remnant diameter ~0.14 deg. = 8.4 arcminutes
       Eyeballing from mosaic fits file, center is abt:
       (RA, dec) = (00:25:17, +64:08:00) (+/- 3 sec RA, +/- 30 arcsec dec)

Projection drawing
------------------
Just initial guesses, to save/play with.

See notes file in data/

Sanity check: if I generate a region and save/reload, will it have the right
rotation? How is this saved with only two points?
Answer: if I save in ds9 format, it appears to store a "thickness" as well.
The "thickness" direction is determined by the ordering of the two points,
so the region's orientation is fully specified.
Yes: projection(x1,y1,x2,y2,width), width is "thickness", great.

When thinking about region sizes/counts:
* SN 1006 has diameter 0.5 deg., vs. ~0.14 deg. for SN 1572
* But count rates are much higher for SN 1572

Pulsar-looking thingy!! (brief diversion)
-----------------------------------------
Whoa!

Thing that looks like Crab Pulsar, located at:
RA, dec = 00:25:00.1, 64:10:44 (hr, deg.).  
Most visible in power scaling, also sinh.

* Rest frequency is between 1-2 keV, assume 1.5 keV --> 3.63e17 Hz.
* Redshifted stuff, 0.7-1 keV, assume 0.85 keV --> 2.06e17 Hz.
* Blueshifted stuff, 2-7 keV.  Don't know this (too large range).

Red frequency shift is 1.57e17 Hz.
From Doppler formula we can get the redshift as:
`(3.63 - 1.57) = (c - v)/(c + v) * 3.63`.
Velocity projected along line of sight is ~0.28c... wow.

Flip to solve for frequency of blueshifted material:
`(3.63 + \Delta f) = (1+0.28)/(1-0.28) * 3.63`
Blue frequency shift is 2.8e17 Hz,
thus blueshifted stuff has freq. ~6.4e17 Hz --> 2.6 keV.

Coooooooooool. But I don't know how to find further information, yet.

[update: now I feel rather silly -- it's probably just ejecta]

Next steps in region processing
-------------------------------

Ask about what we should see after the shock?  Hard to see whether present, or
thermal emission, or what.

* Run around SNR in each band, to check for any spots I missed, and note
  previous regions that look contaminated (in each band).
* Check each region individually for (1) clean in all bands, (2) long enough in
  front and back for fitting.
* Sort the filaments -- get the obviously good ones, then figure
  out how to make use of the blah and bad ones.  Which ones will we use?
* Background subtraction (compare local vs. global background).  I see two ways
  to approach this part.
  1. For each region, have enough extending outwards to estimate immediately
     nearby background.  Subtract mean from each spectrum (in each band
     separately, of course...)
  2. For entire image, we could get a thorough background spectrum (mask the
     SNR, apply a threshold and/or remove point sources [prefer latter to avoid
     losing information]).  Because background spectrum should have frequency
     dependence, this may be important when we generate spectra (checking for
     contamination/other effects).  But, binning in energy integrates over the
     frequency dependence.  So profiles just need a constant shift (?).
* Generating spectra and checking for contamination

2nd order stuff (nice but less important)
* Optimization
* Calculate *expected* curvature based on width of regions
  (idea, could quantify how wide these may be. But, multiple filaments likely
  to be a larger confounding effect)
 

More reproducible / automated profile region selection
------------------------------------------------------

Idea: try testing projections for minimum peak width, e.g. using pyds9.
    or, find when peak height is maximized

Supply initial guess regions, then rotate them about their centers (?) until
the filament peak width is minimized.  You should generate the projections in a
consistent way (just for ease of use/manipulation), e.g. starting from outside
and moving in.

(remember that we get THREE projection plots out, one for each band...)



Thursday, 2014 June 5
=====================

Summary
-------
* Finished selecting regions, color-coded by quality.
* Installed CIAO 4.6 on my computer (omitted HRC background event files)
* Installed ftools, XANADU (incl. XSPEC), XSTAR (HEASOFT 6.15.1)
  from pre-compiled binary
* Installed pyds9, Python interface to ds9 via XPA
* Created script to extract and plot profiles from ds9, using pyds9 interface

ciao4.6, ds9, heasoft-6.15.1 are all installed to `~/bin/`.
For heasoft, I had to use `./configure PERL=/usr/bin/perl`
to prevent Perl mismatch.

Note for background removal -- the black region outside the SNR is absolutely
not uniform -- there is a trend of decreasing counts towards the edge of the
image.

Agenda review
-------------

1. Re-do/tweak regions (wait for higher res images from Brian?)
    * Manual tweaking, see what Rob/Brian say
    * Possible optimization w/ pyds9
    * Check curvature?
2. Background subtraction (wait for discussion / how-to)

So now I want to generate plots of the counts (for first pass
review/discussion), and also start reading up and learning abt supernova
physics anyways (prepare for next stages).

Meanwhile I am installing the various software packages.

Extracting profile parameters using ds9's projection tool
---------------------------------------------------------

Whenever I change the current frame in an rgb image, all the projection plots
are automatically regenerated.  This was very annoying at first, but now I
can exploit this to get the plot data -- as I could not find a way to run the
analysis commands via pyds9/xpa!

:)

Please see iPython notebook in `data/`

TODO for FRIDAY

Improve plot quality (set .matplotlibrc or whatever)
Sort plots into good/bad/whatever


Friday, 2014 June 14
====================

Summary
-------
* Set up boilerplate plotting functionality (improve matplotlib defaults)
* Sent email to Rob/Brian with regions, plots, notes
* Started reviewing Jackson on synchrotron radiation (ch. 14)

pyds9 version of set-up code
----------------------------
Here we go...

    import ds9
    d = ds9.ds9()
    d.set('rgb')
    d.set('rgb red')
    d.set('file 0.7-1kev_mosaic.fits')
    d.set('scale limits 4e-8 2.1e-6')
    d.set('rgb green')
    d.set('file 1-2kev_mosaic.fits')
    d.set('scale limits 4e-8 2.42e-6')
    d.set('rgb blue')
    d.set('file 2-7kev_mosaic.fits')
    d.set('scale limits 3e-8 1.5e-6')
    d.set('rgb lock scale yes')
    d.set('scale histequ');

Agenda for today
----------------
* Twiddle with profile plots/scripts and send regions/plots/notes to Rob/Brian
* Twiddle with CIAO's specextract, just playing now
* Call abt NASA computer access? / email?
* X-ray mtg + GSFC colloq on geodesy (that's probably ~2 hr of your day...)
* Self-study of SNR physics, read Ressler paper

Notes
-----
CIAO looks a little tricky / not so interactive, probably could use some
feedback from Rob/Brian first.  I think learning about SNRs is higher priority
now, so I can better use CIAO/XSPEC/etc later on.

Currently I am setting up some boilerplate plotting material:

1. ~/.matplotlib/matplotlibrc
2. my own `fplot` module for matplotlib. This just prints the settings in use,
   intended for iPython notebooks or similar.  Maybe I can add more utilities
   later.

Okay, not perfect, but looks passable for now.

Sent email to Rob/Brian with regions, annotated regions/labels on Tycho image,
plots of radial intensity profiles, pictures of strange pulsar-like object.



(Week 2) Monday, 2014 June 9
============================

Summary
-------
* Meeting with Brian to look at region picks
* Wrote script (`ds9proj2box.py`) to convert ds9 region files to CIAO
  region files (aggressively made it usable).

What's the agenda for today?  Do some self-study / reading until meeting with
Brian at 11am... then see what to do (start messing with CIAO?).

(some reading about synchrotron radiation / bremsstrahlung ensues)

This is kind of inefficient to be honest.  I guess this is probably low
priority, but I need to pick this up more efficiently...  Do some problems.
Get an intro level astrophysics textbook `w__w`.

Meeting with Brian
------------------
Regions -- just start with 10-12 best ones for now.  Make sure they're good in
all three bands (certify this).

Run `chandra_repro` on raw data
Run `specextract`

We only need 176 kilosec. (longest obs) to check spectra for contamination.
obsID is 10095, `package_1531_140609112030.tar`

Yes, we are using Hughes' 2009 (750 ks) observation.


Work now
--------
Check that the "good" regions are good in all three bands.
Obtain spectra for these regions (and check the funny object while you're at
it) using CIAO.

Now writing script to convert DS9 projections into boxes.

Wow that took way longer than it was supposed to.
1. too sleepy at work...
2. figured out...
    * I don't need great circle formulae (linear approx)
    * Stupid me, RA and dec don't map cleanly onto x-y
    * Rewrite to use physical coords (interface with ds9 to generate and get
      rid of intermediate files)

Some pythonic reminders: to work with files, use:

    with open(...) as f:
        for line in f:
            print line
        # or if writing
        for line in stuff_to_write:
            f.write(line)

Regex is great... but you need to keep in mind the tools in your arsenal, and
reach for the right ones.  I guess it will take time.

Remember to move things to scripts, when needed (i.e., if I have to use a
script more than 1x or 2x?).  Use sysargv.

CIAO wants files in physical coordinates
(see, e.g., [this thread](http://cxc.harvard.edu/ciao/threads/regions/index.html#formats))


Tuesday, 2014 June 10
=====================

Summary
-------
* Read CIAO threads on `chandra_repro`, `specextract`
* Reprocess longest single observation (~176 ks, ObsID 10095)
* Troubleshoot specextract with Brian, generate test spectra
* XSPEC tutorial with Brian

Okay, now that I've got the region file conversion working...
onto generating stuff with specextract

CIAO reprocessing
-----------------

Now running `chandra_repro verbose=5` (it seems like it will take a while) on
the 176 ks observation (obsID 10095).
Finished abt 1 hr, 4 minutes later (`acis_process_events` took ~50 minutes)

    WARNING: Observation-specific bad pixel file set for session use:
        /Users/aarontran/Documents/snr-research/data/chandra/
        10095/repro/acisf10095_repro_bpix1.fits

    Run 'punlearn ardlib' when analysis of this dataset
    completed.

    The data have been reprocessed.
    Start your analysis with the new products in
    /Users/aarontran/Documents/snr-research/data/chandra/10095/repro

Meanwhile I start reading about specextract.

CIAO specextract
----------------

To use a stack, input
`pset specextract infle="acisblah.fits[sky=@../thing.ciaoreg]"`
instead of using the `region(..)` syntax.
But, specextract also demands a stack of equal size for `bkgfile`.

On the flip side, it may be more sensible now to select backgrounds near the
radial profiles themselves, instead of using this mega-thing that will take
forever to run.

Sequence of specextract commands
--------------------------------

    punlearn specextract
    pset specextract \
        infile="acisf10095_repro_evt2.fits \
               [sky=region(../../../thing_tycho.ciaoreg)]"
    pset specextract outroot=extracttest/spec
    pset specextract \
        bkgfile="acisf10095_repro_evt2.fits
                [sky=region(../../../background.ciaoreg)]"
    pset specextract verbose=2

    # These are already set by default
    pset specextract weight=yes
    pset specextract weight_rmf=no
    pset specextract combine=no

    # Ancillary files: aps, mskfile, badpixfile
    # should be automatically found

    # Not set yet cuz I'm clueless
    # energy_wmap, grouptype, binspec, bkg_grouptype, bkg_binspec

    plist specextract
    specextract

What's this `mode=h` stuff, that shows up?  Some threads explaining how to use
`chandra_repro` and `specextract` have this additional flag, but I can't find
its documentation.  Maybe a legacy thing.

This does take a little time, even though just a small region... the background
region was very large, maybe that's a bad idea.
Started around 12:28-12:30pm, 
Stopped at 13:47pm -- this is taking really long.  Could be because I locked my
screen to go grab lunch meanwhile, if that messed it up
It was stuck on mkwarf (made it through sky2tdet after a lonnnnng time)

Try again at 13:58pm.  Stop at 14:24, it's still sitting on mkwarf

Background region
-----------------
There is a decrease in counts away from the SNR, I'm not sure if this is
physical or not (or if this should be accounted for by `chandra_repro`, which
it might not be here).

One thing I could do is to take two concentric annuli of the background, to see
if the spectrum changes as a function of radial distance.

New profiles with 0.5" binning
------------------------------
I.e., no binning
Just re-run script (should get scale limits again)
The new plots look good.

Talked to Brian
---------------
Some troubleshooting:

1. the region I'm extracting spectrum from has too many counts (2 orders of
   magnitude more than, e.g., along rims)
2. background region is way too big -- you want to stay nearby.  How to tell
   quality of background region?  Eyeball... at least hundreds, thousands of
   counts to get a decent spectrum.  We'll look in XSPEC and see how it goes.
3. Idea: just generate a few background circles around the rims, then assign
   each background region to several nearby profiles.
4. Brian can send files with just count numbers, instead of intensity units,
   so I can mess with them / play with them.
5. Download Funtools for DS9, there's a way to get it to load in the menus of
   DS9 by default.
6. Some stuff in CIAO can run in parallel, take advantage of multiple cores.
   Not sure about specextract... (from skimming the page)

Brian explained that the program is generating a response function... response
matrix function or whatever.  Captures the chip's spatial and frequency
response, but must be generated on the fly each time, it's a multiplicative
process.

ALSO, MEETING TOMORROW BEFORE COFFEE/COOKIES THING, PROBABLY AROUND 9AM OR SO
WAKE UP EARLY, SLEEP EARLY, SEND STUFF INTO JVGR EES ASAP...

Trying again, using `testspec.ciaoreg` and `testspec_bg.ciaoreg` (smaller, both
nearby, looking at thermal emission).  Let it run -- and it finishes within a
few minutes!  Very snappy.

Move on to working with XSPEC, stop by Brian's office if
I get it to work! (and he can get me started)

Meddling with spectra
---------------------

I have the following files from a single run of `specextract`:

* `spec.pi`
* `spec_grp.pi`
* `spec_bkg.pi`
* `spec.arf`
* `spec.rmf`
* `spec_bkg.arf`
* `spec_bkg.rmf`

So we have a source spectrum, a grouped source spectrum,
and a background spectrum (`.pi` files).
Then we have weighted response files, for background as well.

REMEMBER, when starting xspec stuff (`heainit`), start before starting CIAO if
you are running in the same session, due to conflicts with parameter files.

See meeting notes for Brian's XSPEC tutorial!


Wednesday June 11
=================

Summary
-------
* Generated "cutback" good regions and test spectra with specextract, XSPEC
  (this took a long time, ~50 minutes for 10 regions + backgrounds)
* Meeting with Rob and Brian -- looked at spectra, discussed subsequent fitting
* Cleaned up files, try to make log more readable in future

Quick list of to-dos
--------------------

Due to the JVGR paper (and time mismanagement yesterday night)
I started actual work today ~12pm, quite late.  So I will be staying
late today, and pounding away to learn Tcl, XSPEC.

For now, what do I need to prepare for the meeting?  I won't be able to throw
together scripts for everything, yet (which I can mention). We need to:

1. assess quality of good and okay region files (plots/profiles already
   generated) (done, but need to do same using spectra)
2. draw up new "good" regions without the background thermal stuff (done)
3. draw up new "okay" regions without the background thermal stuff
4. run `specextract` on the good regions (okay regions too if time)

Cutback regions + background regions made, and converted to CIAO regions
(remember to use script for projection-type regions).

Meeting with Brian and Rob
--------------------------

Showed the spectra for the good regions, nonthermal emission.

Feedback on regions and spectra:

* Region 4 (next to the orange tuft) is really contaminated, welp.  Not gonna
  work.  If that was the only region we had available, then maybe... could try
  to subtract off stuff in the back.  But no, too much trouble.
* Region 3 (wispy NE filament) has some line emission.  Twiddle with the box
  and try to make it better!
* Need to document all of this stuff -- explain what's going on, where we
  looked, what we tried and did.

Looked at some of the southern regions -- yup, contamination is a problem.  I
didn't have spectra for those yet though.

Another extension (different project): something they worked on with another
remnant was to look at the precursor.  Electrons could diffuse ahead of the
shock, give rise to a squished out rim.  Here we are seeing very sharp rim
edges, which constrains some of the physics of those rims.

Iterative sort of process now -- adjust the region, check the spectrum and see
if you can get the Si line to go away.  Rinse and repeat (and log what you're
doing).  Take good notes -- when I'm gone they have to be able to say, "We
checked x region and it looked bad/good for y reason".  So log the bad
regions, log all the region twiddling.

Next step -- fit the spectra to a power law and verify that they look good.

* Power law is a strictly empirical model of course, but simple.  Don't care
  about the actual parameters, as long as it fits it's okay.
* `srcut` is a fitting model in XSPEC -- you supply a radio flux (at 1 GHz)
  and radio spectral index, then fit for the cut-off energy
  (and the spectral index after the knee is determined???).
  So you only get like one or two free parameters.
  Rooted in the physics very nicely, but more hassle.  Especially as radio
  telescopes just don't have the necessary resolution.

Amusingness: apparently Nina is working on the pink fluff, idea is to try
getting the Doppler velocities and constructing a 3-D map.  So we're dividing
and conquering on these supernova remnants.  Nina arriving Monday, can help get
her started on DS9/CIAO/HEASOFT stuff (Rob will cc on email).

Computer is still a question mark, Rob/Brian came to take a look a bit after
the meeting.

So I was underprepared for that meeting, not as much to discuss/present
unfortunately.  Not as many questions going in.

Funtools install
----------------
Installed Funtools 1.4.4 to `~/bin/saord/`.
Made some productivity tweaks (set-up MacVim so I can use splits etc.)
Research notes organization ---------------------------

To better organize my thoughts/work, and reduce clutter in this log file.
(although granted I'm tired right now, I'm having a hard time keeping track of
what needs to be done for research)

1. Create file for code pipeline (list commands that need to be run).
   Will store 1. commands, 2. make evident where I need scripts
2. Create a running agenda

Currently brain-dead (~7pm), I'm going to get food, come back, nap, and maybe
try working through the night.

Looking at all the spectrum plots now:
Regions 1, 3, 5, 6, 8, 9, 10 all have a weak Si line
Regions 2, 7 look relatively clean
Region 4 is very contaminated!  (could try splitting)

Of these regions, 5, 10 look to be a bit noisier.  But not terribly so.
Next, cutback regions for the OKAY/BAD regions, then spectra.


Thursday 2014 June 12
=====================

Summary
-------
* Experimented with specextract pipeline, figured out how to manipulate files
  and headers.  Some fiddling with XSPEC, and XSPEC model fitting, as well.
* Set up new computer!  This took a little while...
* Updated/improved script for making profile plots, made 0.5 arcsec resolution
  plots for all regions (good/iffy/bad)
* Made new background regions, to cover all possible regions
* Formulate pipeline for region/background processing and improvement
* Draft code for specextract scripting


Specextract pipeline
--------------------

New game plan: run specextract on the profiles and backgrounds independently,
and then link them together after the fact.  No redundancy.
From one test: run `specextract` and generate spectra for 1) lone region,
2) lone background, 3) region + background correction; then manually create
4) lone region linked to background using CIAO's `dmhedit`.
The linked spectra and the files all looked good!

(for `dmhedit` commands, see the end of this
[thread](http://cxc.harvard.edu/ciao/threads/extended/index.html#stepbystep)

Useful tools: fv (gui to interact w/ fits files), fdump (dump to plaintext!).
And, remember to use `fhelp name_of_ftool`


Back to ds9 regions
-------------------
Need to look at good/iffy regions and assess amt of thermal contamination.
Then move forward with fitting (1. fit power law, 2. profile fitting)
Updated script for generating profile plots
Now scripting the spectrum generation


Background regions around SNR
-----------------------------

Considerations when selecting background regions:
* CCD chip boundaries!  Visible in RGB image, w/ log scaling
  (bars of anomolously increased counts, cutting across the SNR)
* Point sources, most often in red (0.7-1 keV) or blue (2-7 keV)
* Get as close to SNR as possible
* Try to keep background region sizes similar
* Background regions should be larger than profiles -- to account for much
  smaller number of counts
Generally, use log scaling when drawing boundaries to reveal point sources.

I picked a good number, looking at `profiles_all.reg`.  I may revise them
though.


iMac computer setup
-------------------

New iMac arrived in the afternoon! Spent some time downloading files / setting
up various customizations.  In general I am installing most astro software
to `~/bin/`.  This should be added to `$PATH` in `.bash_profile`.

1. Set Finder, Firefox/Chrome, Terminal preferences (using bash right now...)
2. Set System Preferences (remap Caps Lock key to Ctrl)
3. RC files -- the following ones were imported / set-up

    .bash_profile
    .screenrc
    .vimrc
    .vim/colors/vividchalk.vim
    .matplotlib/matplotlibrc  # UPDATE after 
    .xspec/Xspec.init  # Just need to change the help doc viewers

4. Software:
    * MacVim (update `.bash_profile` alias accordingly)
    * TextMate (enable shell support for `mate` in preferences)
    * XCode (+ command line tools in preferenes)
    * homebrew (compared to macports, doesn't need sudo?)
    * Anaconda (Python distribution with numpy/scipy/matplotlib/iPython/etc.)
    * MacTeX
    * XQuartz (X11 windowing system, needed for subsequent astro software)
    * ds9, pyds9, Funtools
    * CIAO, HEASOFT

I think that is everything necessary so far... we'll see what else comes up.

Friday 2014 June 13
===================

Summary
-------
* Reviewed "good" region selection, created new set of good regions
  (see notes in `data/notes-profiles.md`)
* Ran specextract on new good region selection (17 regions, ~45 min.)
* XSPEC fitting tutorial with Brian
* Mike Werner's Spitzer colloquium (lots of cute/neat science)
* Created script to link regions and background spectra

Meet with Brian today!

Spectra organization
--------------------
Writing script to link backgrounds with regions.
See `pipeline.md` for details.

    data/
    data/spectra/bkg
    data/spectra/good-1
    data/spectra/good-2


(Week 3) Monday 2014 June 16
============================

Summary
-------

* Linked spectra together (`spec_linkbg.py`, `spec_clearbg.py` working)
* Software setup w/Nina (derp derp)
* Drafted script to parse spectra and output plots with PyXspec
* Started reinstalling/troubleshooting HEASOFT install, to get
  PyXspec functionality.


Spectra manipulation
--------------------

Linked spectra in `data/spectra/good-2` to backgrounds in `data/spectra/bkg`
I accidentally messed up the order of arguments to `spec_linkbg.py`
So, I created and executed `spec_clearbg.py` to clean up the BACKFILE entries
for the files in `data/spectra/bkg`.

Fiddling around with XSPEC's commands `identify`, `setplot id`, looking at
spectral line output/information

PyXspec setup -- HEASOFT reinstall
----------------------------------

YES, BEAUTIFUL, THERE IS SUCH A THING AS `PyXSPEC`... makes life easier.
Only possible issues: no implementations of commands: hardcopy, identify.
But I think we'll be okay.

Issues: binary installation of HEASOFT doesn't have the necessary PyXspec
files.  And, there may be trouble finding development header files:

* python is in `/opt/local/.../2.7/bin/python2.7`
* Python.h is in `/opt/local/.../2.7/include/python2.7/Python.h`
* libpython2.7.dylib is in `/opt/local/.../2.7/lib/libpython2.7.dylib`

where `...` shortens `Library/Frameworks/Python.framework/Versions`.
(use `find /opt/local -name [libpython*, Python.h]` to locate)
It should work as it searches relative to the executable's location.
And the configure didn't fail or anything.  But if this is an issue, add
symlinks to the relevant files...


HEASOFT reinstall
-----------------

Reinstall HEASOFT 6.15.1 from source as follows:

    export PERL=/usr/bin/perl
    export FC=gfortran-mp-4.7
    ./configure --prefix=/Users/aarontran/bin/heasoft-6.15.1/

This required:
1. reinstall MacPorts gfortran w/ 32-bit compatibility
   `sudo port install gcc47 +universal` (this takes a while...)
2. Two issues with MacPorts install of Python: could not find library/header
   files (`Python.h`, `libpython2.7.dylib`), and is 64-bit only.  Solutions:
   1) use Apple's Python with `sudo port select python none`.
   2) reinstall Macports Python with `sudo port install python27 +universal`
3. While building/making resource-intensive packages, run:
   `sudo mdutil -a -i [off/on]` to toggle Spotlight indexing.

If using Apple Python, the following issues occur:
1. need to issue bash command `export VERSIONER_PYTHON_PREFER_32_BIT=yes`
   in order to use PyXspec
2. Apple Python doesn't have matplotlib/scipy/other packages set-up
3. Not the version of python used by ipython

If using Macports Python (reinstalled with +universal flag):
1. need to issue command `arch -i386 [i]python` to run PyXspec
2. matplotlib/numpy/scipy aren't 32-bit compatible

Dammit.

Conclusion: PyXspec scripts must be run as, essentially, standalone scripts.
Saved output from said scripts can be passed to other scripts, which use 64 bit
Python.  Not much better than Tcl scripting and I burned about 8 hours on this.

On the bright side, PyXspec is pretty convenient!


Tuesday 2014 June 17
====================

Summary
-------
* Finished HEASOFT reinstall (notes above, merged with yesterday)
* Reviewed "good-2" set of regions for quality

Back to region quality review/inspection.  I now have:
1. spectra with fit parameters and a chi-squared
2. profile plots
3. RGB image of Tycho
to help evaluate the quality of all the selected regions. So I walk through
all the data and determine where else to adjust regions / pare down test/bad
regions.


Wednesday 2014 June 18
======================

Summary
-------
* Started installing software on iMac (see install log there)
* Cleaned up `regions-good-2.reg` and made `regions-good-3`.  See
  `notes-regions.md` for information on region rejections / twiddling.
* Ran pipeline on `regions-good-3` -- first complete test of pipeline.


Region (`regions-good-3`) selection
-----------------------------------

move backgrounds farther away and see if things change....
Start profile fitting.

Compile the fit statistics for all the regions.
Set up the computer...


Thursday 2014 June 19
=====================

Summary
-------
* Fixed HEASOFT install on iMac, and matplotlib/iPython problems with libpng
* Generate `background-2` regions and link spectra (`good-3`)
* Some reading about synchrotron rim physics (Cassam-Chenai, 2007, etc.)
* Began profile fitting.  Tested a two-exponential model with middling results


Friday 2014 June 20
===================

Summary
-------
* More twiddling with profile fits
* Meeting with Rob, discussed profile fitting
* Twiddled profile fits and tidied profile fitting script


Profile fitting uncertainties
-----------------------------
After talking to Rob in the morning, I estimate the uncertainties as follows.
Go into the unbinned mosaics, look at background noise.  The numbers seem to
appear in roughly discrete units.  Estimate the smallest discrete unit (of
course there is slight variation), and take that to be one count.  Do this for
all three bands.

I use this to convert flux/intensity to counts in the profile fitting notebook.
The values are:

    0.7-1 keV: 8e-9 intensity units
    1-2 keV: 2.8e-9 intensity units
    2-7 keV: 5e-9 intensity units

I'm very perplexed by the 0.7-1 keV units (although, truthfully, it does seem
to be in line with the amount of noise/spread of signal).


Profile fitting functions
-------------------------

Start with a two-exponential model:

    h_{\mathrm{up}}(x) &= A_u \exp \left(\frac{x_0-x}{w_u}\right) + C_u \\
    h_{\mathrm{down}}(x) &= A_d \exp \left(\frac{x - x_0}{w_d}\right) + C_d

The upstream/downstream split occurs at `x=x_s`, and I use `x_s` as a fit
parameter instead of `A_d`.  (we can chose to remove either `A_d` or `A_u` from
the fit).  For sanity, the parameters are:

    xs, x0, Cu, Cd, wu, wd, Au, Ad

and only seven of these parameters are free, because we demand continuity.
Here I list some approaches/settings that I've used:

0. Single step fit for everything, with decent initial guesses.  I used
   original regions-good-3 data (i.e., no thermal upswing in the back).
   This is the data / fit I brough to show Rob in the morning:

1. Two-step fit.  Freeze the split location xs, and fit.  Then unfreeze xs and
   try another fit.  This seems to work pretty well.

2. Introduce data with thermal counts in the back (good-3-allback).
   Now, in addition to two-step fit: smooth data and find local minimum nearest
   to the synchrotron peak.  Use this to bound the fitting domain.

3. As above, but also fit background terms Cu, Cd on first fit attempt.
   Didn't make things too much better, but less blowup.

4. Let Au be a fitting parameter instead of Ad -- rationale was that, since we
   have more consistent upstream data, it should be easier to fit Au and solve
   for Ad.  Still freeze xs, Cu, Cd on first fit, allow all to vary in the
   second fit.

This didn't seem to do a lot (Au goes to zero, blows up, or nails it).

5. Add x0 to the list of frozen parameters (freeze: x0, xs, Cu, Cd) in first
   fit.  Again, let all go free in second fit.

THIS WORKS WELL!

6. Now remove Cu,Cd from first fit freeze.  So only freeze x0, xs.
   This is okay.  For reference, let me note the initial guesses here

    # Freeze: xs, x0
    xs_fr = x_of_max + 0.5  # Smoothing shifts peak slightly left
    x0_fr = xs_fr + 0.5  # xs left of x0 in good fit (empirically speaking)

    # Free parameters: wu, Au, wd
    Cu_g = np.mean(y_fit[-3:])  # The obvious guess
    Cd_g = np.mean(y_fit[:3]) / 2.  # Usually shoots under trough
    wu_g = 0.5  # Upstream width
    wd_g = 2.5  # Downstream width
    Au_g = np.amax(y_fit)/2.  # Upstream amplitude
    init_guess = [Cu_g, Cd_g, wu_g, wd_g, Au_g]

Now, I want to improve some of the rattier fits.  So far this procedure isn't
too reproducible / well-documented, I'm just playing it by ear here.

Pushing the frozen value of xs forward, helps improve red fits slightly.
Only a few red regions are going NaN on me now.

    # Freeze: xs, x0
    xs_fr = x_of_max + 1.5  # Smoothing shifts peak slightly left
    x0_fr = xs_fr + 0.5  # xs left of x0 in good fit (empirically speaking)

    # Free parameters: wu, Au, wd
    Cu_g = np.mean(y_fit[-3:])  # The obvious guess
    Cd_g = np.mean(y_fit[:3]) / 2.  # Usually shoots under trough
    wu_g = 0.5  # Upstream width
    wd_g = 2.5  # Downstream width
    Au_g = np.amax(y_fit)/3.  # Upstream amplitude
    init_guess = [Cu_g, Cd_g, wu_g, wd_g, Au_g]

I think we are hitting diminishing marginal returns.  I'm very uncomfortable
with the instability of the fits.  It's probably time to consider another
fitting procedure...


Sunday 2014 June 22
===================

Summary
-------
* Further fitting fiddling


Summary of work on profile fitting should probably be moved to its own log
file, soon.

Anyways, to continue....

1. Try a two exponential model, but add one more parameter -- change x0 to xu
   and xd.  Try letting both go free, first.

   Holy smokes that really sucked.  Okay, if I do this, the approach would have
   to be -- freeze split, fit both sides, get a chi2.  Step split location
   until I find a minimum in chi2 space.

2. Implement a stepping approach for the decoupled two exponential model.
   Ahh, but this is difficult because the continuity requirement
   still forces them to be linked.

   Anyways I try this -- step xs over a range of numbers, getting the best fit.
   Then, unfreeze xs and let it run free too -- but in practice this doesn't
   do anything, because we've already let it run free.

   It looks, honestly, terrible.
   In chi^2 space, it tracks out a parabola like shape.
   But on rare occasion, it will jump to a better trajectory
   and give a much better fit.

   But we don't have a systematic way to reach said trajectory!  Here's an
   example of a good fit from the better trajectory (the best one in this test)

    red fit (xs frozen): xs = 22.04, xu =  21.5, xd = -235.1,
    Cu = 0.3992, Cd = 1.598, wu = 0.981, wd = 1.492, Au = 8.589
    chi2 =  35.7, chi2red =  0.51

   It looks like the "coupled" 2-exponential model still does better, actually,
   with good initial guesses (chi2red = 0.50 instead).  And, it doesn't crash
   and burn on everything else.

So, let's give up on this decoupled model.  Adding another free parameter is
just making things worse.


(Week 4) Monday 2014 June 23
============================

Summary
-------
* Morning meeting with Rob, Brian, Nina (will make a regular thing)
* Updated good-3 spectra to link to background-2 spectra
* Added adjustments to XSPEC fitting (excise or fit silicon line)
* Updated `ds9projplotter.py` to parse arbitrary bands/labels/data/etc
* Generated new plots of 


More fitting stuff:
Calculate FWHM for those peaks where the data indicate a FWHM may be estimated
We need to revise the regions and the fit routine.

Guess: the error from overshooting the peak location, will be kind of large.
Possibly larger than that from shifting the profile around.

Brian short discussion:
* Quantifying Si line emission: one way is to give an "equivalent width"
(e.g. as in Winkler 2014).
* Splines -- yeah, fitting is better.  But you can try...


New energy band mosaics
-----------------------
(from Brian, today)
Solely breaking up 2-7 keV band

2-4, 4-7 keV
2-3, 3-4.5, 4.5-7 keV

Ad hoc flux-count conversions, for new bands
--------------------------------------------
* (2-3 keV) count <--> 4.5e-9 units
* (2-4 keV) count <--> 4.5e-9 units
* (3-4.5 keV) count <--> 4e-9 units
* (4-7 keV) count <--> 6e-9 units
* (4.5-7 keV) count <--> 6.5e-9 units


Tuesday 2014 June 24
====================

Summary
-------
* Twiddled profile fitting notebook to work on arbitrary bands/labels/data
* Tested further fit functions...
* Implemented (ugly, ugly code!) FWHM uncertainty estimation
* Calculated FWHM uncertainties.  But, not tested/debugged (and I know it is
  buggy)


More profile fitting antics
---------------------------

Try a two exponential function with split at x = x0, discard free parameter xs.
Now fiddle with initial guesses.  It looks possibly workable!  But, would need
to twiddle with the fits...

Try the Ressler function (With the split at x = x0).  This is going nuts.
It is having a hard time converging (even with 4x default max iterations
[maxfev], 8000 instead of 2000)

Revelation: the fits for simple cases (two exponential, two exponential
simplified) spewing out 'inf's are due to having errors of ZERO which kills
everything!  Ad hoc solution.


FWHM uncertainties
------------------

Trying to decode what Satoru did.
The scaling function goes bonkers for large positive xi, and goes bonkers for
negative xi (except, really small negative xi)

Using 50 arcsec. instead of 200 arcsec.
A tolerable range of xi to check seems to be [-1,0] and [0, 10]
But this depends on the region -- some are much stiffer than others.

Okay, kind of working now.


Wednesday 2014 June 25
======================

Summary
-------
* FWHM testing, and further work on the stretching function (to 1. verify FWHM
  values, and 2. understand behavior/effects of parameters)
* FWHM values and uncertainties, I think, are now reliable.


Profile uncertainties
---------------------
Satoru's uncertainty calculation: generate a new energy-band image (NOT
corrected for vignetting or exposure time) and count the number of photons in
each area.

Brian: will look for count mosaics, or generate new ones


Stretching function
-------------------
On one hand, this is almost as simple as you can get -- it's just a parabolic
stretch, instead of a linear one.  On the other hand, bad behavior occurs
easily when the stretch is extended too far.

With ad-hoc flux estimates for 0.7-1 keV band, the FWHM uncertainty in the
0.7-1 keV band is enormous!  In fact, if we didn't have the distortion that
occurs at large xi, the uncertainty would be even higher -- so we are
underestimating the uncertainty.


Thursday 2014 June 26
=====================

Summary
-------
* Generate count images + profile data from `merged_evt.fits`
* Compute uncertainties from photon count images instead of my flux-to-count
  estimates (not much difference)
* Slight tweaks to XSPEC fitting (constrain Si line fit, more reproducible and
  physically meaningful/relevant)
* Evaluate regions using profile + spectrum fits, and adjust to create
  "good-4-ext" set of regions.  No new regions added yet.


Count files
-----------
List of commands (just for record-keeping)
(also changing all instances of `4p5` to `4.5` for consistency...)

    dmcopy "merged_evt.fits[EVENTS][energy=700:1000][bin x=3300:4900:1,y=3300:4900:1]" 0.7-1kev_counts.fits
    dmcopy "merged_evt.fits[EVENTS][energy=1000:2000][bin x=3300:4900:1,y=3300:4900:1]" 1-2kev_counts.fits
    dmcopy "merged_evt.fits[EVENTS][energy=1000:1700][bin x=3300:4900:1,y=3300:4900:1]" 1-1.7kev_counts.fits
    dmcopy "merged_evt.fits[EVENTS][energy=2000:7000][bin x=3300:4900:1,y=3300:4900:1]" 2-7kev_counts.fits
    dmcopy "merged_evt.fits[EVENTS][energy=2000:3000][bin x=3300:4900:1,y=3300:4900:1]" 2-3kev_counts.fits
    dmcopy "merged_evt.fits[EVENTS][energy=2000:4000][bin x=3300:4900:1,y=3300:4900:1]" 2-4kev_counts.fits
    dmcopy "merged_evt.fits[EVENTS][energy=3000:4500][bin x=3300:4900:1,y=3300:4900:1]" 3-4.5kev_counts.fits
    dmcopy "merged_evt.fits[EVENTS][energy=4000:7000][bin x=3300:4900:1,y=3300:4900:1]" 4-7kev_counts.fits
    dmcopy "merged_evt.fits[EVENTS][energy=4500:7000][bin x=3300:4900:1,y=3300:4900:1]" 4.5-7kev_counts.fits

From these, generate profile data with COUNTS only and extract the relative
errors (remember to account for integration length)

Spectrum processing
-------------------
Brian: to bound the linewidth, think about e.g., a Doppler broadening of
~10000 km/s.  In two directions that's 20000 km/s ~ 0.07c.  Energy 1.85 keV *
0.07 is 0.13 for a FWHM = 2.35 sigma, so sigma ~ 0.05 keV is as broad as you'll
get.

See the code in `spec_fitplot.py`, I have changed soft/hard limits on Si line
fit energies and sigmas (LineE strictly in [1.75,1.95] keV, sigma strictly less
than 0.1 keV).  This seems to constrain the lines very well.  No attempt to
fit the sulfur line at this time.

XSPEC equivalent width: value range is ~0.02 keV to 0.2 keV (typically 0.02 to
0.1 keV, only one case of 0.2 keV).

Region selection
----------------
See entry for `good-4-ext` in `data/notes-regions.md` to explain how new regions
are picked.

About 2/3ths of way through adjusting regions I accidentally hit ctrl-w one too
many times and killed my DS9 window.  So time to redo it...


Friday 2014 June 27
===================

Summary
-------
* Finish generating `good-ext-4` regions.  Generate profile data, and `good-4`
  without the thermal upticks
* Compute power-law exponents for each region (no averaging), estimate
  uncertainties in quadrature
* Test other profile fits -- smoothed (not-)interpolating spline, Gaussian +
  ramp function.
* Refactored profile fitting code for my sanity

QUALITATIVELY: using 1-1.7keV counts instead of 1-2 keV counts doesn't appear
to make much difference.

Uncertainties are still very large on FWHMs -- uncerts are comparable to those
for SN 1006, but the FWHMs are so much smaller...


Short meeting with Brian
------------------------
Went to Brian to discuss (1) how the exponents (`m_e`) were calculated in the
Ressler paper.  New regions, but basically almost the same as before.

Salient issues:
1. HUGE range of FWHM values being averaged together, to give a FWHM value,
   with small uncertainties (comparable to adding together / adding in
   quadrature) in the Ressler paper.
2. Relatively large uncertainties on computed FWHMs for Tycho (order 10-100%),
   vs order 1-10% for SN 1006.

Therefore, 

1. Test different profiles and see what happens (profiles need to hit the peak
   points, should not undershoot)
2. Go through output, see how many regions have positive/negative/flat trends
   in FWHMs.
3. Email Satoru and Sean, ask for some profiles to test our fitting routine
   on.  Should check that we get similar results, and similar uncertainties.
   This would confirm that the analysis is sensible.

So be prepared to bring this all in on Monday for discussion.

Profile fitting experiments
---------------------------

Verdict on Gaussian + ramp: doesn't work.  Has trouble accounting for steep
fall-offs and undershoots the maximum data values often.  But, it fits easily
and is not terrible.

How can I document / keep track of all these good/bad/middling fits?
So, the regions I'm keeping the region files to recreate them.
For the fits -- I need to refactor the code, and be able to regenerate them on
the fly.  So that's okay.  It would be good to record some comparative FWHMs /
uncertainties / chi-squares...


(Week 5) Monday 2014 June 30
============================

Summary
-------
* Checked SN 1006 profile fits from Satoru
* Fixed FWHM uncertainty calculation (after SN 1006 fits)
* List other ways to fit rim widths (functional models, + check literature)
* Calculate FWHMs from splines and manual-capping of max, to compare values
* Further refactor/clean up code

Catalog of regions
------------------
(n.b. gauged using old, large uncertainties -- now less relevant...)

How many show a downward, flat/upward trend?  Within error.
Looking at simplified, two exponential fits + errors on FWHMs; 4 band split
(0.7-1keV, 1-1.7 keV, 2-4 keV, 4-7 keV).

* Downwards: 1, 2?, 3?, 5?, 6?, 7?, 8, 9, 10, 11?, 12
* Flat: 4? except for 0.7-1kev, 13

Cases where at least two pts could be flat even:
1, 2, 3, 4, 5, 6, 10, 11, 13
i.e., most regions.

As noted before, the exponential fits have a risk of over/undershooting the
data peak.  So the FWHMs may not be reliable; I have marked those that may be
affected with a question mark (strictly by eyeballing).

In general, only a few have latitude for a FLAT trend; the rest may have 2 data
points side-by-side or so, but usually there is a decrease somewhere.  A few
cases (those without question marks) are unequivocal decreases.


Fitting checks (morning meeting)
--------------------------------
Main issue is uncertainties on FWHMs, now.  See meeting notes

Fitting Satoru's SN1006 profiles
--------------------------------

Observations:
* Satoru seems to have moved the peaks to all lie at ~50 arcsec
* The fit domain lengths are quite variable, though all strictly less than
  200 arcsec, it seems.  All the files he sent are 200 arcsec long,
  but the fit domains were cut appropriately in the paper

            0.7-1 kev band       1-2 kev band        2-7 kev band
------------------------------------------------------------------
Region 08   23.8 +2.0/-1.5      20.9 +1.0/-0.8      15.9 +0.8/-0.9
Region 10   33.4 +1.5/-1.3      30.7 +0.5/-0.5      26.8 +0.7/-0.6
Region 16   74.0 +5.2/-5.1      63.6 +2.1/-2.0      46.3 +2.3/-2.3

### Fitting using my procedure (as follows)

Procedure:
* I set the fit domain by eyeballing, to try to match the Ressler paper
* To work with my fit routines, I rotate regions 8, 10 (multiply by -1, and add
  100) to get them to work.

My uncertainty calculation will differ from Satoru's, because of the way I'm
moving the data around (not setting peak to one location, all facing same way,
et cetera).  If the uncertainties disagree, then these details may matter.

To follow my procedure, I translate/flip all profiles to start at ~0 with the
downstream side (closer to SNR) having smaller radial position.

Verdict: errors ~1 order of magnitude larger than what Satoru gets (!!!). WAT.
Absolute values of FWHMS are okay, although there is some disagreement.

            0.7-1 kev band       1-2 kev band        2-7 kev band
------------------------------------------------------------------
Region 08   28.4  +23/-12       25.0  +11/-8.3      22.8  +21/-13
Region 10   33.4  +5.6/-11      29.6 +4.5/-6.5      25.2 +6.4/-8.2
Region 16   73.4  +20/-54       63.4  +18/-28       44.9  +23/-25

Let's nail down what's going on.


Uncertainty calculation fix
---------------------------
(determined after looking at chi-squared criterion in paper, and looking at how
XSPEC does its chi-squared error estimation)

The `$\Delta \chi^2 = 2.7$` applies to the regular chi-squared, NOT reduced
chi-squared!!!!!  Please refer to Section 14.5 of Numerical Recipes (1st ed.)
(Section 15.6 of 3rd ed.).  So that makes things much better.

Now, the table (from above, using my procedure) looks like:

            0.7-1 kev band       1-2 kev band        2-7 kev band
------------------------------------------------------------------
Region 08   28.4 +1.4/-1.4      25.0 +0.8/-0.9      22.8 +1.3/-1.5
Region 10   33.4 +0.9/-1.1      29.6 +0.6/-0.9      25.2 +0.9/-1.1
Region 16   73.4 +4.1/-4.3      63.4 +2.2/-2.2      44.9 +2.9/-2.5


Rob: and that's why we do this, and beat our heads against the wall.
The region sizes are probably okay, remember the point is to just get
enough signal...


Fitting routine improvements
----------------------------
Goals:
1. Improve fitting to avoid overshooting on rim peaks.  Ideally, different fits
   should give comparable FWHMs and energy dependence.
2. Better characterize uncertainty

The exponential best captures the steep rims, just tends to overshoot too much.

### Improvements
* Pin FWHM calculation -- fix the maximum at the maximum observed data point,
  then calculate the FWHMs from that

> done: this makes for a helpful comparison

* Stretch in y-direction, see how that affects uncertainties?
  Could also allow stretch in both x,y directions.  Then uncertainties are
  determined by ellipse in parameter space, marking Delta-chi-squared ~ 4.71

> not done: likely won't do this; stretching in x-coord would have biggest
> effect on FWHM already, to 1st order, y-coord stretch will not change FWHM

### Possible functions
* Convolve 2-exponential model w/ some Gaussian or smoothing kernel
  How to fit something like this?

> Briefly tested -- convolving may work w/ fit, but makes it hard to solve for
> FWHM values since discrete convolution gives discrete data

* Some kind of penalty function

> Tested: cap on profile fit height kinda helps, although I suspect this
> worsens the fit and the height limit has no real grounding

* Hand fitting with two exponentials, manipulating/stepping parameters
  to get something that looks nice (no justification)
* Hand fitting with Ressler model, manipulating Gaussian to get good peak
* Fit a Fourier series
* Fourier filter a spline function (same vein as above)
  [example?](http://bigwww.epfl.ch/publications/unser9301.pdf)
* Cusp function (e.g., `$y = x^{2/3}$`), may not be steep enough
* Lorentzian? (likely not steep enough)
* Analytic, physical function (e.g., Berezhko and Voelk, 2004)

### Literature
* Araya et al.(2010) -- Gaussian fit to Cas A filaments
* Ballet (2006) -- no fits, no data
* Vink/Laming (2003) -- no fits, just eyeballed
* Parizot et al. (2006) -- fit exponential to downstream side, then use
characteristic lengthscale `R` to compute width as `w \approx 4.6 R`.
* Berezhko/Voelk (2004) -- analytically compute a long function (can try this?)
* Rettig/Pohl (2012) -- nab characteristic values from other papers!
* Bamba et al. (2003) -- SN 1006, two-exponential model (almost the same!)
  doi:10.1086/374687
* Bamba et al. (2005) -- multiple SNRs, two-exponential model
  doi:10.1086/427620

Experimenter bias: different fits will give very different FWHMs.  So it is
tempting to choose the fit function that, qualitatively or quantitatively,
gives the best/most consistent TREND in energy dependence.
But, there isn't really a good justification/confirmation for anything.


Tuesday 2014 July 01
====================

Summary
-------
* Fixed manual implementation of cap on fit function height
  (now verified to match calculation of FWHM, from imposing fixed "max" on FWHM
  calculation, for fit functions that overshoot data maxima)
* Calculating FWHMs for several function fits gives sizeable spread
* Reviewed list of possible improvements/functions (immediately above)
* Downloaded Kepler observations
* Quick calculation of power law fits to all band data to get `m_E`


Status of fitting calculations
------------------------------
Now calculating FWHMs by pinning either FWHM max or function to top of data.
Definite spread in FWHM values.

I favor two exponential model, simplified, over the two exponential model with
a free split.
Two exponential model with split usually gives FWHMs almost identical to
that of simplified 2-exp model.  In cases where FWHMs differ (model w/ split
gives smaller FWHM), the model w/ split has higher reduced-chi-squared -- at
least just looking at regions 2, 3 of regions "good-ext-4".


Questions/concerns on `$m_e$` calculation and interpretation
------------------------------------------------------------

* Which model do we favor, now?  Given the spread, especially when considering
  or ignoring the position of the data maximum.
  (and, I am thinking that constant offsets matter -- subtract background?)
* What to do about the spread in `m_e`?  How to average numbers for Sean's
  model?
* (to self -- continue to get an idea of the literature, and read what has been
  done before on nonthermal x-ray emission / filaments from SNRs)
* (keep an eye on the number of weeks left...)

sean's motivation for doing band-to-band fits instead of a 3 point power law
fit?

What went down in SN 1006?  Was there a lot of variability in each region,
before it got averaged out?



Wednesday 2014 July 02
======================

Summary
-------
* Polished up code for global fits to all data, for `m_E` calculation
* Started FORTRAN 77 tutorial
* Careful reading of Ressler paper, to understand variables/calculations in
  Sean's code (needs a few more careful readings).
* Push code/material to Github


For now... figure out what Sean's doing with the FWHMs.
Estimate mE for SN1006.
Figure out what the model code is doing.

Need to make small fixes to region picks and all that stuff!!

Lots of things to address by next Monday.



TESTING SEAN'S CODE:

with default resolution, B0=113, eta2=1.1, mu=1
it runs in fit mode pretty fast!
tring in plot mode, it didn't finish for about 16 minutes and I ctrl-c'd it.
So trying again while I run to get dinner, to see what it does.

Left it to run for about 1hr 20 minutes, nothing happened -- it was still just
spewing stuff onto the screen.

So plot mode looks like it does what it says -- calculate a ton of crap for
plots (I don't know what yet).


Thursday 2014 July 03
=====================

Summary
-------
* Discussion with Brian about Sean's email, paper calculations, next
  deliverables, questions, lots of misc. things
* Set up network printers on iMac!
* Finished bits of FORTRAN tutorial
* Walked through and cleaned up Sean's model code (`FullEfflength_mod.f`)
  (reformatted spacing, added many comments)
* Started debugging process (intensity profile output looks incorrect, from
  Sean's unmodified code)
* Sean/Steve/Brian/Rob email chain on model calculations

Today:
1. a little more time on FORTRAN and playing with Sean's code.
2. twiddle fit stuff, make spectra, etc.  Multiple small patches
    (a) in here -- run sean's analytic code, cuz that's really simple and
    straightforward.  Maybe do it manually, just to get some numbers
    that Sean/Steve/Brian/Rob can interpret/discuss!
    (b) new regions, new things, blah blah blah.
3. Make figures of regions, spectra in a multi-page pdf and send around to
   Brian, Rob, Sean, Steve, (Satoru?)
3. tabulate things, explain/write up things in LaTeX file
4. send things to brian/rob

Debugging notes
---------------

Ah, so the output from the fortran thing (both mine, and Sean's original)
suggests there's an error in the FWHM calculation ("box length error")
Add comma to variable declarations in FWHM subroutine? (between xmax/xmin)

I need to knife this thing with implicit nones everywhere.  Or just port it to
Python, or something.  For now, I just want to get it working so I can get some
numbers out.  Approach -- walk through code and comment EVERYTHING, then dive 
in to fix selected pieces (e.g. the box length error bug).  Then slowly add
improvements.  Not all at once.

Goal: for the FWHMs/`m_E` values reported for SN1006, be able to reproduce all
of Sean's calculated eta/B0 values.

I am getting strange results on the following inputs:
* B0 = 110d-6
* eta2 = 0.1
* mu = 1
* using fit mode (inu=2 and I set rminarc=100d0)
* default resolutions, as suggested

The code spews out numbers (something is going wrong), and the output is quite
strange.  Intensity profile output monotonically decreases with increasing
radius, from whatever the scaled radius is (here, 0.88 to 1); the 2nd column
(presumably 2 keV band) is all NaNs.  The FWHM calculations are being forced
all the way to the edge (i.e., it can't find a half-max!) -- and thus the FWHM
error, but really the error seems to be in whatever is generating the profiles.


(Week 6) Monday 2014 July 07
============================

Summary
-------
* Checked `m_E` calculations for SN 1006
* Sanity checks on `m_E` averaging, intensity vs. count unit usage
* Further clean-up to profile fitting notebook (separated FWHM computation
  and FWHM analysis/fitting)
* Began reimplementing / testing Sean's code for fitting equation (6)
  (catastrophic dump transport equation)


SN 1006 `m_E` values
--------------------
See notebook.  Verdict -- huge variability in `m_E` values as well, but
averaging FWHMs and computing `m_E`, or averaging `m_E` values, seems to be
reasonably consistent!

Computation of average `m_E`
----------------------------
A quick calculation: averaging the FWHMs then calculating `m_E` gives:

    m_E = \frac{1}{\log\left(E_2/E_1\right)} \log\left(
        \frac{ \sum_{i=1}^n \mathrm{FWHM}_{i,2} }
             { \sum_{i=1}^n \mathrm{FWHM}_{i,1} } \right)

I.e., we compute `m_E` using an _arithmetic_ average of the FWHM values.
Here we have `n` distinct regions; the 2nd subscript in 1, 2 denotes lower or
higher energy band respectively (equivalent to primed/unprimed variables, in
Sean's paper).

If we calculate `m_E` for each region separately, then average the exponents:

    m_E = \frac{1}{\log\left(E_2/E_1\right)} \log\left[ \left(
        \frac{ \prod_{i=1}^n \mathrm{FWHM}_{i,2} }
             { \prod_{i=1}^n \mathrm{FWHM}_{i,1} } \right)^{1/n} \right]

I.e., we compute `m_E` using a _geometric_ average of the FWHM values, this
time.

I'm not sure if that's helpful, but maybe something to keep in mind -- which
one would better capture the energy scaling?  Ultimately we want the best proxy
for the real physics, what's really going on, as independent of measurement
variation as we can get.

Looking at [Wikipedia](http://en.wikipedia.org/wiki/Geometric_mean#Properties)
and the relevant reference [Comm. ACM](http://dx.doi.org/10.1145/5666.5673),
I do think the geometric mean is appropriate.

Another intuitive/sketch argument: if we are averaging together small/large
numbers, random variation/uncertainty in large FWHMs could easily hide or smear
out the effects of variation in small FWHMs, when we know that the quantity of
interest is the _"normalized" variation_ between FWHMs, which should be
consistently calculable, to comparable uncertainties regardless of the size of
the FWHMs.

Filament 1 of SN 1006 shows this best (regions 1-4, not including 6).
The `m_E` calculated from the geometric mean is -0.39, vs. -0.16 from the
arithmetic mean (between 0.7-1, 1-2 keV).  Here region 2 shows a sharp drop in
rim width, but the FWHM values are quite small.  So when averaged, the larger
FWHM values predominate; as they show a smaller change in width with energy, so
the magnitude of `m_E` is smaller.  But, the geometric mean recovers the
scaling of region 2 -- weighting it equally, in a sense.

(note, we haven't considered errors/uncertainties here -- this also matters).


Calculating `m_E` with intensity units
--------------------------------------

Using total counts vs. intensity flux units, gives very different values for
`m_E` point to point, and some variation in FWHM calculation.  To check this, I
compute FWHM values from count units and flux units, and compare the FWHM
changes in different energy bands (similar to what was done for 1-1.7 keV vs.
1-2 keV).

Result: it seems like there could be a small shift in FWHM value.  But, the
FWHM shifts are distributed about zero, both positive and negative, and the std
deviation is much larger than the mean.  So I think we can neglect this, and
say that it doesn't matter much.  Besides, we should use intensity units rather
than count units (because of the exposure/vignetting correction).


iPython notebook cleanup
------------------------
Damnit this is always cringe-inducing to stare at.

* Moved fit functions to a separate module (but, fit routines which supply
  initial guesses, run multiple fits with frozen parameters, etc are kept
  within main notebook).
* Moved main functionality out into separate method, to minimize global
  namespace pollution...  now, just run ONE cell and it will populate
  the region dictionary with information (fits, fwhms, etc)
* Brian on software maintenance vs. research time: something we all deal with,
  right?  Remember to think about cost-benefit analysis, how much time it's
  gonna take.  Put in the time necessary to make it work for you.
  And, will you reuse it many times, or is it a one-off thing?

Main change: moved FWHM fitting functionality (all calculation of `m_E` and
similar) to new notebook.  Profile fitting notebook now outputs plaintext and
serialized objects (Python pickles) storing information about regions.


Plot of region profiles and spectra
-----------------------------------
Basically, mimicking Figure 8 of Ressler et al.
Some remarks from quick drop-in with Brian, today.
* Plot spectra and fits out to 0.5 keV, to show minimal oxygen line emission.
  Steve (Reynolds) has been worried about thermal emission, need to show that
  spectra are clean.
  Oxygen line is at about 0.56 keV.  Expect Tycho to have relatively little
  oxygen (a fraction of a solar mass).
* No need to do the two spectra as for SN 1006.  We don't have enough room
  behind the rim to get clean (nonthermal) spectra, and what they tried to show
  in SN 1006 (that rims do thin with increasing energy), is somewhat redundant.
  (and, not quantitative anyways, without spectral indices)
* Yes, go ahead and make a nice multiple panel plot now -- do once and be done
  with it


Sean's analytic model fitting (equation (6))
--------------------------------------------
Migrated Sean's code to iPython notebook to parse FWHM-energy values.
Slight cosmetic changes / moved things around, but otherwise should be the same
(need to provide a version of code that validates this).


Tuesday 2014 July 08
====================

Summary
-------
* Generated preliminary Tycho numbers from Sean's approx/analytic fitting code
* Rearranged directory structure for clarity
* Updates to profile fitting code
* Added/tested script to split regions at FWHMs/fit edges, for spectra


Fits to equation 6
------------------
Some brief remarks.

1. I'm seeing fields of order 0.1 to 1 mG (mostly 0.1 to 0.5)
2. If I use "capped" FWHM measurements, and/or change the distance estimate
   from 2.3 kpc to ~4 kpc, I get slightly smaller numbers -- but I haven't
   quantified this, just eyeballing.  I cannot tell whether fits are better
   with capped FWHM measurements or not.
   Fits in general are not very good -- large residuals abound.
3. Concern from yesterday -- errors, as computed for confidence intervals by
   varying parameters (exploring chi-squared space), are quite large!
   Especially compared to 1-sigma from the parameter standard deviations.

At least, qualitatively, the numbers are consistent with what Parizot et al.
(2006) give (~250 to 500 microgauss)


Directory re-organization
-------------------------
To more easily track where files are located, and to allow for easier
experimentation/extension to the pipeline structure.
Pipline organization idea borrowed from Software Carpentry lecture on data
management: [link](http://software-carpentry.org/v4/data/mgmt.html).

WARNING: this breaks links between spectrum files.

My proposed directory layout follows.  It will likely be updated (because
that always happens) but I hope the general structure can be consistent.

    data/ *.fits
          README.md
          notes-regions.md  # Describe how regions are selected

          region-ID/ README.md  # Describe how spectra/profiles are derived
                     *.reg      # from region files (scripts, dates, cmds)
                     *.ciaoreg
                     *.physreg
                     *.jpg  # Images of regions on SNR, etc
                     *.pdf  # Multipanel plot of spectra/profile

                     spectra/ *.pi  # IF multiple sets, push to folders
                              *.rmf
                              *.arf
                              plots/ *.pdf   # Plots of spectra/fits
                                     *.ps
                              fits/ *.txt  # Fitting information
                                    *.qdp  # from XSPEC, etc.

                     profiles/ *.dat  # If multiple sets, push to folders

                               plots/ *.pdf
                                      *.png
                               fwhms/ *.pkl
                                      *.txt
                                      plots/ *.pdf  # e.g. Fig. 10 of Ressler
                                      model/ *.txt  # Modeling results?

          background-ID/ README.md  # as for regions
                         *.reg
                         *.ciaoreg
                         spectra/ *.pi
                                  *.rmf
                                  *.arf

    code/ regions/
          spec/
          profiles/
          models/

Or something similar.  Exact arrangement of profiles/fwhms stuff TBD.


Updates to profile fitting
--------------------------
* Define some magic constants (smoothing parameters, labels) in "configuration
  cell" and output config log to document code inputs/outputs.
* Serialize fit function as SOURCE CODE, instead of function object, for
  interoperability (else pickle file requires access to function's
  containing module).  Also opens opportunity to use JSON for data exchange.
* Improved output for spectra generation and FWHM processing farther along the
  pipeline.  Note that the Python pickle (serialization) also includes the
  fitting function and best fit parameters, so that does document the fit used.


Spectra generation
------------------
Spectra locations will be set by FWHM and profile fit locations.
After computing FWHMs and profile fit domains (smoothed local minimum cut),
notebook/script saves cut and FWHM locations to pickle/text files.

A script should read the cut/FWHM locations for all regions,
generate TWO new region files -- one with fronts, one with backs,
then use ds9proj2box.py to convert to CIAOREG, then feed through specextract
chain.


ds9projsplitter.py + pipeline updates
-------------------------------------

Got script `ds9projsplitter.py` working.  After discussion with Brian, we
decided to plot spectra from two regions, kind of following what Satoru did in
Sean's paper.  Updated `ds9proj2ciao.py` as well.  Some misc. notes follow:

* Running into issues with multiple copies of ds9 opening, again.  Once ds9 has
  been opened/closed in a Python session, it won't open again (can't find the
  ds9 instance).
  STUPID SOLUTION: reload the ds9 module (`reload(ds9)`).
* Added blacklist to profile fit code (convert FWHMs to NaNs if blacklisted)
  Blacklisting guideline: if FWHM could not be calculated from 2exp fit, or
  the spline/capped fits could not get a FWHM (i.e., without overshooting we
  could not get a FWHM), add to blacklist.  Kind of adhoc and subject to bias,
  but good enough.
* Cut locations are determined in profile fit code, so they can be saved to
  region dictionary for plotting later on.
* Updated XSPEC fitting script to fit 0.5-7 keV as requested by Brian (confirm
  no oxygen line at 0.56 keV).

Also ran specextract on the upstream(rim) / downstream regions.


Wednesday 2014 July 9
=====================

Summary
-------
* Improved XSPEC fits + fit output
* Plotted spectra from upstream/downstream split on regions-4
* Generated first iteration(s) of multipanel plots for distribution


XSPEC fitting output
--------------------
Script now generates:
1. plain XSPEC log file as before (with equiv. width + 90% conf. errors)
2. json file with fit parameters, errors, fit statistic
3. npz file with spectrum data+errors, folded model, and background

Also added option to generate 90% conf. errors, as it is time intensive + error
prone + may require user input.

Some notes from debugging/etc:
* matplotlib.pyplot doesn't work with 32 bit python -- need to reinstall
  matplotlib and build from --universal installs of libpng, freetype libraries
* After much finagling -- to get spectrum data, use xspec.Plot object
  attributes (which returns the values used for plotting).
  Spectrum, model attributes give raw data for channels, not energy bins.

Result of upstream/downstream divide
------------------------------------

From regions-4, it looks like the rims are pretty clear of thermal emission.
Downstream of the rims is fairly contaminated -- silicon line is usually
visible, often sulfur as well, and some other lines.

The plots don't seem to be scaled by area -- units are "normalized counts per
second, per keV".  Satoru's plots (Figure 8) have units "cts/s /keV /cm^2".
How do you do that in XSPEC??  One link about this normalization issue:
[link](http://heasarc.gsfc.nasa.gov/xanadu/xspec/manual/XspecWalkthrough.html).
Not very helpful.  In PyXspec, spectrum object attribute `areaScale` is 1.0,
which I suppose is the EFFAREA keyword discussed (ctrl-f on "normalization").

It seems like we must scale by detector effective area -- how do we do this?

Downstream regions, with contamination, can't get 90% confidence errors.
The reduced chi-squared values are too large, so XSPEC errors out.

Plotting
--------
Usual spiel -- play with opacity, color, plot markers.  I think it looks pretty
nice.

My default matplotlibrc (fontsize 9 everywhere) is good for pdf output,
hopefully for publication ready figures.  But, it is not so good for viewing
output within a notebook.  So that needs tweaking, depending on how we present
this.


Thursday 2014 July 10
=====================

Summary
-------
* Tried generating plots of FWHM dependence for multiple regions together
* Walkthrough of script for generating spectra from all 750ks of Chandra data
* Added rich images/text etc to profile/spectrum plotting notebook


Region + Tycho images
---------------------
Need to use box regions, to get rid of dashed lines (projections).

1. convert to ciaoreg
2. apply format changes (save as -img.reg, with text regions)
3. crop RGB tycho image
(center on RA, dec = 6.334 deg, 64.137 deg; size 0.17 x 0.17 deg)

Try: 6.324, 64.143 (ra, dec in deg)
0.165 deg square


Plotting notebook
-----------------
Add plots of FWHM dependence, nice images of Tycho regions, tables of FWHMs
(using snippets of HTML).




Friday 2014 July 11
===================

Summary
-------

Plotting notebook updates
-------------------------
Fixed tickmarks and stuff
Discussion with Brian about FWHM presentation

Hide cells / reformat output?  If it takes ~half an hour sure but not if it
takes a half a day.


Specextract scripts
-------------------
Twiddle paths/setup to work

Acting on regions-4-up and regions-4-down,
the script took about 1 hr, 50 minutes for each set.
So, in total, expect ~4 hr (including set-up time).
Don't let the computer sleep while running.

Plotting notebook changes
-------------------------
Many changes -- Discussion with Brian yesterday and today)
Simple way to think about it -- we have a 3-D space with the axes:
* region / filament number (i.e., arbitrary ordering whatever)
* FWHM measurement
* energy / energy band


More SNR data (Kepler, Cas A)
-----------------------------
Redownloaded one of the ObsIDs and finished reprocessing everything.
Now, testing out `merge_obs` using:

    merge_obs "6714/repro,6715/repro,6716/repro,6717/repro,6718/repro,7366/repro" out/ bands="csc" verbose=3

I don't know what energies to pick, to evaluate the exposure map...
Started at 8:41pm, Friday July 11, computer set to not sleep.
Finished at 8:57pm?  Dang, that was actually pretty fast

Pretty images!... man.  But they're too small.

FOOL.  You forgot to set the bin size to 1, no wonder!...  Try again at 9:18p
Finished 22:34 (so, ~1.5 hr. okay, not terrible).

Download Cas A meanwhile.

Saturday 2014 July 12
=====================

Using grppha with group min 15 on the total spectra...
Ran the rest of the pipeline on the spectra, twiddled scripts.

Profile plotting notebook now has full spectra + fixes suggested by Brian from
Friday.  I have a super, super hacky workaround for the website issue --
just use

    ipython nbconvert --to html plotter_prfs_specs.ipynb

Then go in and search for `<div>` cells like:

    <div class="text_cell_render border-box-sizing rendered_html">

    <div class="cell border-box-sizing text_cell rendered">
    <div class="cell border-box-sizing code_cell rendered">

I'm very perplexed because the `</div>` tags seem to be crazy mismatched.
(and, I can't believe that pandoc is just embedding images inline...)

Okay need to clean up these notes soon but I think I'm done for today.


Monday 2014 July 14
===================

Continuing issue - how to best deal with FWHM presentation?
* Normalize averaged values, or average normalized values
* Error calculation -- Sean's procedure, or std. err of mean

Brian suggests using Sean's procedure for now for consistency, discuss with
Rob/Steve/Sean/Satoru what to do.

Note -- textmate doesn't properly fold all div tags...??? I'm not sure
actually, pandoc's nbconvert output is just a mess.
Add width to notebook-container div tag, to manipulate webpage width (I set it
to 1240px for now...)

Emailed website to Steve/Sean/Brian/Rob

So need to
1. overhaul specextract pipeline (handle region coordinates correctly, test and
   ensure it works correctly with my adjustments)
2. rehaul data structuring for FWHMs because it's a pain in the ass to plot /
   organize
3. include switches to test different measurements/procedures, for
   reproducibility.
4. proper config files / cmd line arguments for fitting/FWHM analysis scripts?
5. clean up rsch notes document... keep track of what changes/etc I've been
   making

Code -- how to refactor, how to best manage, how to ensure reproducibility?
Mulled on this a while

Tuesday 2014 July 15
====================

Summary
-------
* AAAAAAAAAAAAAA
* Code writeup, review of models and physics, sent to Brian
* Figured out discrepancy with Sean's usage of diffusion coeff eta2 in code


Code refactoring -- stared, sighed, and moved on to Sean's model code again
(which is both more doable and more fun)
k

Start writeup of code -- finally figured out the weird variables sean uses,
although it doesn't explain his choice of variables when explaining parameter
space (but fortunately NO ONE CARES).


Wednesday 2014 July 16
======================

Summary
-------
* Reviewed most calculations in Sean's code (e- distribution functions TBD)
* Debugged Sean's code to a point where it seems to work
* Emailed Sean with various questions...

Slow hard debugging of emissivity integrals etc...
Anyways I cleaned up the emissivity code
Happily, it's now giving sensible numbers!!!
My test case is B0=0.0001, mu=1, eta2=1
I get 33.45" at 1.00 keV, 26.85" at 2.00 keV
with m\_E at 2.00 keV being -0.32

The output intensity plots and electron distributions look very nice!
For this very simple case.  I don't have a sense of when/how it will blow up,
yet.  And I haven't run any timing tests.

Started looking into the synchrotron spectrum numbers and stuff...
Checked out Shu, Shu, and Pacholczyk
